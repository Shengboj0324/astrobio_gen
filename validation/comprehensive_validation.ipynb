{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import zarr\n",
    "import xarray as xr\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "from data_build.metadata_db import MetadataManager, DataDomain, StorageTier\n",
    "from surrogate import get_surrogate_manager, SurrogateMode\n",
    "from datamodules.cube_dm import CubeDM\n",
    "from pipeline.pipeline_run import PipelineRunner\n",
    "from validation.eval_cube import CubeEvaluator\n",
    "from models.datacube_unet import CubeUNet\n",
    "from models.surrogate_transformer import SurrogateTransformer\n",
    "from models.graph_vae import GVAE\n",
    "from utils.config import load_config\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸš€ Comprehensive Scientific Data Validation Suite\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "print(f\"System: {os.uname().sysname} {os.uname().release}\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ValidationConfig:\n",
    "    \"\"\"Configuration for validation suite\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_runtime_minutes = 30  # Maximum runtime\n",
    "        self.sample_size_limit = 1000  # Samples per domain for testing\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Performance thresholds\n",
    "        self.min_r2_score = 0.95\n",
    "        self.max_latency_ms = 400\n",
    "        self.max_energy_error = 0.01  # 1%\n",
    "        self.max_mass_error = 0.005   # 0.5%\n",
    "\n",
    "        # Data domains to validate\n",
    "        self.domains = [\n",
    "            DataDomain.ASTRONOMICAL,\n",
    "            DataDomain.EXOPLANET,\n",
    "            DataDomain.ENVIRONMENTAL,\n",
    "            DataDomain.PHYSICS,\n",
    "            DataDomain.OPTICAL,\n",
    "            DataDomain.PHYSIOLOGICAL,\n",
    "            DataDomain.BIOSIGNATURE\n",
    "        ]\n",
    "\n",
    "        # Model types to test\n",
    "        self.model_types = [\n",
    "            \"CubeUNet\",\n",
    "            \"SurrogateTransformer\",\n",
    "            \"GVAE\",\n",
    "            \"FusionModel\"\n",
    "        ]\n",
    "\n",
    "config = ValidationConfig()\n",
    "print(f\"âœ… Validation configuration loaded\")\n",
    "print(f\"   Max runtime: {config.max_runtime_minutes} minutes\")\n",
    "print(f\"   Sample limit: {config.sample_size_limit} per domain\")\n",
    "print(f\"   Device: {config.device}\")\n",
    "print(f\"   Domains: {len(config.domains)}\")\n",
    "print(f\"   Model types: {len(config.model_types)}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize metadata manager\n",
    "metadata_manager = MetadataManager()\n",
    "\n",
    "# Get database statistics\n",
    "stats = metadata_manager.get_dataset_statistics()\n",
    "print(f\"ðŸ“Š Database Statistics:\")\n",
    "print(f\"   Total datasets: {stats['total_datasets']}\")\n",
    "print(f\"   Total experiments: {stats['total_experiments']}\")\n",
    "print(f\"   Total data chunks: {stats['total_data_chunks']}\")\n",
    "print(f\"   Total size: {stats['total_size_tb']:.2f} TB\")\n",
    "\n",
    "# Check data availability for each domain\n",
    "domain_data = {}\n",
    "for domain in config.domains:\n",
    "    datasets = metadata_manager.query_datasets(domain=domain.value)\n",
    "    domain_data[domain] = datasets\n",
    "    print(f\"   {domain.value}: {len(datasets)} datasets\")\n",
    "\n",
    "print(f\"\\nâœ… Found data for {len([d for d in domain_data.values() if d])} domains\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ValidationRunner:\n",
    "    \"\"\"Main validation runner class\"\"\"\n",
    "\n",
    "    def __init__(self, config: ValidationConfig):\n",
    "        self.config = config\n",
    "        self.results = {}\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # Initialize surrogate manager\n",
    "        self.surrogate_manager = get_surrogate_manager()\n",
    "\n",
    "        # Initialize evaluator\n",
    "        self.evaluator = CubeEvaluator()\n",
    "\n",
    "        # Performance monitoring\n",
    "        self.performance_data = []\n",
    "\n",
    "    def check_time_limit(self) -> bool:\n",
    "        \"\"\"Check if we're within time limit\"\"\"\n",
    "        elapsed = (time.time() - self.start_time) / 60\n",
    "        return elapsed < self.config.max_runtime_minutes\n",
    "\n",
    "    def create_synthetic_data(self, domain: DataDomain, num_samples: int = 100) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Create synthetic data for domains without available datasets\"\"\"\n",
    "\n",
    "        np.random.seed(42)  # Reproducible results\n",
    "\n",
    "        if domain == DataDomain.ASTRONOMICAL:\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 64, 64, 32)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 64, 64, 32)),\n",
    "                'metadata': {'type': 'stellar_atmosphere', 'resolution': '64x64x32'}\n",
    "            }\n",
    "        elif domain == DataDomain.EXOPLANET:\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 128, 64, 16)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 128, 64, 16)),\n",
    "                'metadata': {'type': 'planetary_atmosphere', 'resolution': '128x64x16'}\n",
    "            }\n",
    "        elif domain == DataDomain.ENVIRONMENTAL:\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 256, 128, 64)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 256, 128, 64)),\n",
    "                'metadata': {'type': 'climate_datacube', 'resolution': '256x128x64'}\n",
    "            }\n",
    "        elif domain == DataDomain.PHYSICS:\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 32, 32, 32)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 32, 32, 32)),\n",
    "                'metadata': {'type': 'physics_simulation', 'resolution': '32x32x32'}\n",
    "            }\n",
    "        elif domain == DataDomain.OPTICAL:\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 2048)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 2048)),\n",
    "                'metadata': {'type': 'optical_spectra', 'resolution': '2048_channels'}\n",
    "            }\n",
    "        elif domain == DataDomain.PHYSIOLOGICAL:\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 500)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 500)),\n",
    "                'metadata': {'type': 'biosignature_vectors', 'features': 500}\n",
    "            }\n",
    "        else:\n",
    "            # Generic data\n",
    "            return {\n",
    "                'inputs': np.random.uniform(0, 1, (num_samples, 100)),\n",
    "                'targets': np.random.uniform(0, 1, (num_samples, 100)),\n",
    "                'metadata': {'type': 'generic', 'features': 100}\n",
    "            }\n",
    "\n",
    "    def load_validation_data(self, domain: DataDomain) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load validation data for a domain\"\"\"\n",
    "\n",
    "        datasets = metadata_manager.query_datasets(domain=domain.value)\n",
    "\n",
    "        if not datasets:\n",
    "            print(f\"   âš ï¸  No datasets found for {domain.value}, creating synthetic data\")\n",
    "            return self.create_synthetic_data(domain, self.config.sample_size_limit)\n",
    "\n",
    "        # Use first available dataset\n",
    "        dataset = datasets[0]\n",
    "\n",
    "        try:\n",
    "            if dataset.zarr_path and Path(dataset.zarr_path).exists():\n",
    "                # Load from zarr\n",
    "                zarr_data = zarr.open(dataset.zarr_path, mode='r')\n",
    "\n",
    "                # Sample data to fit within limits\n",
    "                total_samples = zarr_data.shape[0] if hasattr(zarr_data, 'shape') else len(zarr_data)\n",
    "                sample_indices = np.random.choice(\n",
    "                    total_samples,\n",
    "                    size=min(self.config.sample_size_limit, total_samples),\n",
    "                    replace=False\n",
    "                )\n",
    "\n",
    "                if hasattr(zarr_data, 'inputs') and hasattr(zarr_data, 'targets'):\n",
    "                    inputs = zarr_data.inputs[sample_indices]\n",
    "                    targets = zarr_data.targets[sample_indices]\n",
    "                else:\n",
    "                    # Assume single array, split for inputs/targets\n",
    "                    data = zarr_data[sample_indices]\n",
    "                    inputs = data\n",
    "                    targets = data  # Self-supervised for now\n",
    "\n",
    "                return {\n",
    "                    'inputs': inputs,\n",
    "                    'targets': targets,\n",
    "                    'metadata': dataset.dimensions or {}\n",
    "                }\n",
    "            else:\n",
    "                print(f\"   âš ï¸  Zarr path not found for {dataset.name}, creating synthetic data\")\n",
    "                return self.create_synthetic_data(domain, self.config.sample_size_limit)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error loading {dataset.name}: {e}, creating synthetic data\")\n",
    "            return self.create_synthetic_data(domain, self.config.sample_size_limit)\n",
    "\n",
    "# Initialize validation runner\n",
    "validator = ValidationRunner(config)\n",
    "print(f\"âœ… Validation runner initialized\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add validation methods to ValidationRunner class\n",
    "def validate_model_on_domain(self, model_type: str, domain: DataDomain) -> Dict[str, Any]:\n",
    "    \"\"\"Validate a model on a specific domain\"\"\"\n",
    "\n",
    "    print(f\"   ðŸ” Validating {model_type} on {domain.value}...\")\n",
    "\n",
    "    # Load validation data\n",
    "    start_time = time.time()\n",
    "    data = self.load_validation_data(domain)\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    inputs = data['inputs']\n",
    "    targets = data['targets']\n",
    "\n",
    "    print(f\"     ðŸ“Š Data shape: {inputs.shape} -> {targets.shape}\")\n",
    "    print(f\"     â±ï¸  Load time: {load_time:.2f}s\")\n",
    "\n",
    "    # Get appropriate model\n",
    "    try:\n",
    "        if '4d' in model_type.lower() or 'cube' in model_type.lower():\n",
    "            surrogate_mode = SurrogateMode.DATACUBE\n",
    "        else:\n",
    "            surrogate_mode = SurrogateMode.SCALAR\n",
    "\n",
    "        model = self.surrogate_manager.get_model(surrogate_mode)\n",
    "\n",
    "        if not model:\n",
    "            print(f\"     âš ï¸  No model available for {surrogate_mode}\")\n",
    "            return {'status': 'no_model', 'error': f'No model for {surrogate_mode}'}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ Error loading model: {e}\")\n",
    "        return {'status': 'model_error', 'error': str(e)}\n",
    "\n",
    "    # Run validation\n",
    "    results = {'status': 'success', 'model_type': model_type, 'domain': domain.value}\n",
    "\n",
    "    try:\n",
    "        # Performance test\n",
    "        latencies = []\n",
    "        batch_size = min(self.config.batch_size, len(inputs))\n",
    "\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "\n",
    "            start_time = time.time()\n",
    "            predictions = model.predict(batch_inputs)\n",
    "            latency = time.time() - start_time\n",
    "\n",
    "            latencies.append(latency * 1000 / len(batch_inputs))  # ms per sample\n",
    "\n",
    "            if not self.check_time_limit():\n",
    "                break\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_latency = np.mean(latencies)\n",
    "        p95_latency = np.percentile(latencies, 95)\n",
    "\n",
    "        results.update({\n",
    "            'avg_latency_ms': avg_latency,\n",
    "            'p95_latency_ms': p95_latency,\n",
    "            'samples_processed': len(inputs),\n",
    "            'load_time_s': load_time\n",
    "        })\n",
    "\n",
    "        # Accuracy test (sample a subset for speed)\n",
    "        test_indices = np.random.choice(len(inputs), size=min(100, len(inputs)), replace=False)\n",
    "        test_inputs = inputs[test_indices]\n",
    "        test_targets = targets[test_indices]\n",
    "\n",
    "        test_predictions = model.predict(test_inputs)\n",
    "\n",
    "        # Convert to numpy if needed\n",
    "        if hasattr(test_predictions, 'cpu'):\n",
    "            test_predictions = test_predictions.cpu().numpy()\n",
    "        if hasattr(test_targets, 'cpu'):\n",
    "            test_targets = test_targets.cpu().numpy()\n",
    "\n",
    "        # Calculate accuracy metrics\n",
    "        pred_flat = test_predictions.flatten()\n",
    "        target_flat = test_targets.flatten()\n",
    "\n",
    "        r2 = r2_score(target_flat, pred_flat)\n",
    "        rmse = np.sqrt(mean_squared_error(target_flat, pred_flat))\n",
    "        mae = mean_absolute_error(target_flat, pred_flat)\n",
    "\n",
    "        results.update({\n",
    "            'r2_score': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'passes_r2_threshold': r2 >= self.config.min_r2_score,\n",
    "            'passes_latency_threshold': p95_latency <= self.config.max_latency_ms\n",
    "        })\n",
    "\n",
    "        print(f\"     âœ… RÂ² = {r2:.4f}, RMSE = {rmse:.4f}, Latency = {p95_latency:.1f}ms\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     âŒ Validation error: {e}\")\n",
    "        results.update({'status': 'validation_error', 'error': str(e)})\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_comprehensive_validation(self) -> Dict[str, Any]:\n",
    "    \"\"\"Run comprehensive validation across all domains and models\"\"\"\n",
    "\n",
    "    print(f\"\\nðŸš€ Starting comprehensive validation...\")\n",
    "    print(f\"   Max runtime: {self.config.max_runtime_minutes} minutes\")\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for domain in self.config.domains:\n",
    "        if not self.check_time_limit():\n",
    "            print(f\"   â° Time limit reached, stopping validation\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\nðŸ“Š Validating domain: {domain.value}\")\n",
    "        domain_results = {}\n",
    "\n",
    "        # Test available models for this domain\n",
    "        for model_type in self.config.model_types:\n",
    "            if not self.check_time_limit():\n",
    "                break\n",
    "\n",
    "            result = self.validate_model_on_domain(model_type, domain)\n",
    "            domain_results[model_type] = result\n",
    "\n",
    "        all_results[domain.value] = domain_results\n",
    "\n",
    "    # Summary statistics\n",
    "    summary = self.generate_summary(all_results)\n",
    "\n",
    "    return {\n",
    "        'results': all_results,\n",
    "        'summary': summary,\n",
    "        'performance_data': self.performance_data,\n",
    "        'validation_time': time.time() - self.start_time\n",
    "    }\n",
    "\n",
    "# Add methods to ValidationRunner\n",
    "ValidationRunner.validate_model_on_domain = validate_model_on_domain\n",
    "ValidationRunner.run_comprehensive_validation = run_comprehensive_validation\n",
    "\n",
    "print(\"âœ… Validation methods added to ValidationRunner\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add summary generation method\n",
    "def generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate summary statistics\"\"\"\n",
    "\n",
    "    summary = {\n",
    "        'total_validations': 0,\n",
    "        'successful_validations': 0,\n",
    "        'avg_r2_score': 0,\n",
    "        'avg_latency_ms': 0,\n",
    "        'domains_tested': len(results),\n",
    "        'models_tested': len(self.config.model_types),\n",
    "        'passes_all_thresholds': True\n",
    "    }\n",
    "\n",
    "    r2_scores = []\n",
    "    latencies = []\n",
    "\n",
    "    for domain_name, domain_results in results.items():\n",
    "        for model_type, result in domain_results.items():\n",
    "            summary['total_validations'] += 1\n",
    "\n",
    "            if result.get('status') == 'success':\n",
    "                summary['successful_validations'] += 1\n",
    "\n",
    "                if 'r2_score' in result:\n",
    "                    r2_scores.append(result['r2_score'])\n",
    "                if 'p95_latency_ms' in result:\n",
    "                    latencies.append(result['p95_latency_ms'])\n",
    "\n",
    "                # Check thresholds\n",
    "                if not result.get('passes_r2_threshold', True):\n",
    "                    summary['passes_all_thresholds'] = False\n",
    "                if not result.get('passes_latency_threshold', True):\n",
    "                    summary['passes_all_thresholds'] = False\n",
    "\n",
    "    if r2_scores:\n",
    "        summary['avg_r2_score'] = np.mean(r2_scores)\n",
    "        summary['min_r2_score'] = np.min(r2_scores)\n",
    "        summary['max_r2_score'] = np.max(r2_scores)\n",
    "\n",
    "    if latencies:\n",
    "        summary['avg_latency_ms'] = np.mean(latencies)\n",
    "        summary['max_latency_ms'] = np.max(latencies)\n",
    "        summary['min_latency_ms'] = np.min(latencies)\n",
    "\n",
    "    summary['success_rate'] = summary['successful_validations'] / max(1, summary['total_validations'])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Add method to ValidationRunner\n",
    "ValidationRunner.generate_summary = generate_summary\n",
    "\n",
    "print(\"âœ… Summary generation method added\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run the comprehensive validation\n",
    "print(f\"\\nðŸš€ STARTING COMPREHENSIVE VALIDATION\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "validation_results = validator.run_comprehensive_validation()\n",
    "\n",
    "print(f\"\\nâœ… VALIDATION COMPLETE\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Total time: {validation_results['validation_time']:.2f} seconds\")\n",
    "print(f\"Total validations: {validation_results['summary']['total_validations']}\")\n",
    "print(f\"Successful validations: {validation_results['summary']['successful_validations']}\")\n",
    "print(f\"Success rate: {validation_results['summary']['success_rate']:.2%}\")\n",
    "print(f\"Average RÂ² score: {validation_results['summary']['avg_r2_score']:.4f}\")\n",
    "print(f\"Average latency: {validation_results['summary']['avg_latency_ms']:.1f}ms\")\n",
    "print(f\"Passes all thresholds: {validation_results['summary']['passes_all_thresholds']}\")\n",
    "\n",
    "# Create detailed results table\n",
    "results_data = []\n",
    "\n",
    "for domain_name, domain_results in validation_results['results'].items():\n",
    "    for model_type, result in domain_results.items():\n",
    "        results_data.append({\n",
    "            'Domain': domain_name,\n",
    "            'Model': model_type,\n",
    "            'Status': result.get('status', 'unknown'),\n",
    "            'RÂ² Score': result.get('r2_score', 0),\n",
    "            'RMSE': result.get('rmse', 0),\n",
    "            'MAE': result.get('mae', 0),\n",
    "            'P95 Latency (ms)': result.get('p95_latency_ms', 0),\n",
    "            'Samples': result.get('samples_processed', 0),\n",
    "            'Load Time (s)': result.get('load_time_s', 0),\n",
    "            'Passes RÂ² Threshold': result.get('passes_r2_threshold', False),\n",
    "            'Passes Latency Threshold': result.get('passes_latency_threshold', False)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\nðŸ“Š DETAILED VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
