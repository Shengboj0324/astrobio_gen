{
  "summary": {
    "total_attention_classes": 19,
    "with_issues": 4,
    "with_warnings": 9,
    "with_flash_support": 3,
    "with_sdpa_support": 1,
    "with_kv_cache": 3,
    "with_scaling_factor": 8,
    "correct_scaling": 8
  },
  "results": [
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "AttentionType",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [
        "No forward method found"
      ],
      "warnings": []
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "SOTAAttentionConfig",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [
        "No forward method found"
      ],
      "warnings": []
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "FlashAttention3",
      "attention_type": "flash",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "self.head_dim ** -0.5",
      "has_flash_support": true,
      "has_sdpa_support": true,
      "has_fallback": true,
      "has_kv_cache": true,
      "kv_cache_correct": true,
      "issues": [],
      "warnings": []
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "RingAttention",
      "attention_type": "ring",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found",
        "Attention mask dtype handling not found",
        "Attention mask shape handling not found"
      ]
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "SlidingWindowAttention",
      "attention_type": "sliding_window",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "self.head_dim ** -0.5",
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": []
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "LinearAttention",
      "attention_type": "linear",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found",
        "Attention mask dtype handling not found"
      ]
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "MultiQueryAttention",
      "attention_type": "multi_query",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "self.head_dim ** -0.5",
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "Attention mask dtype handling not found"
      ]
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "SOTAAttentionRouter",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": true,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found",
        "Attention mask dtype handling not found",
        "Attention mask shape handling not found",
        "KV-cache implementation may be incomplete"
      ]
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "SparseAttention",
      "attention_type": "sparse",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "self.head_dim ** -0.5",
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": []
    },
    {
      "file_path": "models\\sota_attention_2025.py",
      "class_name": "SOTAAttention2025",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": true,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found",
        "Attention mask dtype handling not found",
        "Attention mask shape handling not found",
        "KV-cache implementation may be incomplete"
      ]
    },
    {
      "file_path": "models\\attention_integration_2025.py",
      "class_name": "AttentionUpgradeManager",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [
        "No forward method found"
      ],
      "warnings": []
    },
    {
      "file_path": "models\\sota_features.py",
      "class_name": "FlashAttention",
      "attention_type": "flash",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "dim_head ** -0.5",
      "has_flash_support": true,
      "has_sdpa_support": false,
      "has_fallback": true,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": []
    },
    {
      "file_path": "models\\hierarchical_attention.py",
      "class_name": "AttentionScale",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [
        "No forward method found"
      ],
      "warnings": []
    },
    {
      "file_path": "models\\hierarchical_attention.py",
      "class_name": "CrossScaleAttention",
      "attention_type": "cross",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found"
      ]
    },
    {
      "file_path": "models\\hierarchical_attention.py",
      "class_name": "PhysicsConstrainedAttention",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found"
      ]
    },
    {
      "file_path": "models\\hierarchical_attention.py",
      "class_name": "HierarchicalAttentionSystem",
      "attention_type": "standard",
      "has_scaling_factor": false,
      "scaling_factor_correct": false,
      "scaling_factor_value": null,
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "No explicit scaling factor found"
      ]
    },
    {
      "file_path": "models\\rebuilt_llm_integration.py",
      "class_name": "GroupedQueryAttention",
      "attention_type": "standard",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "self.head_dim ** -0.5",
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": [
        "Attention mask dtype handling not found"
      ]
    },
    {
      "file_path": "models\\rebuilt_llm_integration.py",
      "class_name": "MemoryOptimizedMultiHeadAttention",
      "attention_type": "standard",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "math.sqrt(self.head_dim)",
      "has_flash_support": true,
      "has_sdpa_support": false,
      "has_fallback": true,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": []
    },
    {
      "file_path": "models\\performance_optimization_engine.py",
      "class_name": "MemoryEfficientAttention",
      "attention_type": "standard",
      "has_scaling_factor": true,
      "scaling_factor_correct": true,
      "scaling_factor_value": "math.sqrt(self.head_dim)",
      "has_flash_support": false,
      "has_sdpa_support": false,
      "has_fallback": false,
      "has_kv_cache": false,
      "kv_cache_correct": false,
      "issues": [],
      "warnings": []
    }
  ]
}