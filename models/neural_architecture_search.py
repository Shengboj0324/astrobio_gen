#!/usr/bin/env python3
"""
Neural Architecture Search for Astrobiology Research
===================================================

Advanced Neural Architecture Search (NAS) system for automatically discovering 
optimal model architectures for climate modeling, atmospheric dynamics, and 
biological system analysis.

Features:
- Differentiable Architecture Search (DARTS) for efficient search
- Progressive Neural Architecture Search for resource efficiency
- Multi-objective optimization for accuracy vs. efficiency
- Task-specific architecture search for different planet types
- Hardware-aware architecture optimization
- Evolutionary search for complex architectures
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Callable
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod
import random
import copy
import math
from collections import defaultdict
import pickle
import json

# Configure logging
logger = logging.getLogger(__name__)

@dataclass
class NASConfig:
    """Configuration for Neural Architecture Search"""
    # Search space configuration
    max_layers: int = 20
    min_layers: int = 3
    max_channels: int = 512
    min_channels: int = 32
    channel_multipliers: List[int] = None
    
    # Search algorithm configuration
    search_algorithm: str = "darts"  # darts, progressive, evolutionary
    max_epochs: int = 50
    population_size: int = 20
    mutation_rate: float = 0.1
    crossover_rate: float = 0.7
    
    # Multi-objective optimization
    objectives: List[str] = None  # accuracy, latency, memory, flops
    objective_weights: List[float] = None
    
    # Hardware constraints
    max_latency_ms: float = 100.0
    max_memory_mb: float = 1000.0
    max_flops: int = 1000000000
    
    # Task-specific settings
    task_type: str = "climate_modeling"  # climate_modeling, atmospheric_dynamics, biology
    input_shape: Tuple[int, ...] = (5, 32, 64, 64)
    output_shape: Tuple[int, ...] = (5, 32, 64, 64)
    
    def __post_init__(self):
        if self.channel_multipliers is None:
            self.channel_multipliers = [1, 2, 4, 8]
        if self.objectives is None:
            self.objectives = ["accuracy", "latency"]
        if self.objective_weights is None:
            self.objective_weights = [0.8, 0.2]

class ArchitectureSearchSpace:
    """Search space for neural architectures"""
    
    def __init__(self, config: NASConfig):
        self.config = config
        
        # Define operation types
        self.operations = {
            'conv3x3': lambda C_in, C_out: nn.Conv3d(C_in, C_out, 3, padding=1),
            'conv5x5': lambda C_in, C_out: nn.Conv3d(C_in, C_out, 5, padding=2),
            'conv1x1': lambda C_in, C_out: nn.Conv3d(C_in, C_out, 1),
            'separable_conv3x3': lambda C_in, C_out: SeparableConv3d(C_in, C_out, 3, padding=1),
            'separable_conv5x5': lambda C_in, C_out: SeparableConv3d(C_in, C_out, 5, padding=2),
            'dilated_conv3x3': lambda C_in, C_out: nn.Conv3d(C_in, C_out, 3, padding=2, dilation=2),
            'maxpool3x3': lambda C_in, C_out: nn.Sequential(nn.MaxPool3d(3, stride=1, padding=1), nn.Conv3d(C_in, C_out, 1)),
            'avgpool3x3': lambda C_in, C_out: nn.Sequential(nn.AvgPool3d(3, stride=1, padding=1), nn.Conv3d(C_in, C_out, 1)),
            'identity': lambda C_in, C_out: nn.Identity() if C_in == C_out else nn.Conv3d(C_in, C_out, 1),
            'attention': lambda C_in, C_out: SelfAttention3D(C_in, C_out),
            'skip_connect': lambda C_in, C_out: SkipConnection3D(C_in, C_out),
        }
        
        # Define activation functions
        self.activations = {
            'relu': nn.ReLU(),
            'gelu': nn.GELU(),
            'swish': nn.SiLU(),
            'leaky_relu': nn.LeakyReLU(0.2),
            'elu': nn.ELU(),
        }
        
        # Define normalization methods
        self.normalizations = {
            'batch_norm': lambda C: nn.BatchNorm3d(C),
            'group_norm': lambda C: nn.GroupNorm(min(32, C), C),
            'layer_norm': lambda C: nn.LayerNorm(C),
            'instance_norm': lambda C: nn.InstanceNorm3d(C),
        }
        
        logger.info(f"Initialized search space with {len(self.operations)} operations")
    
    def sample_architecture(self) -> Dict[str, Any]:
        """Sample a random architecture from the search space"""
        num_layers = random.randint(self.config.min_layers, self.config.max_layers)
        
        architecture = {
            'layers': [],
            'channels': [],
            'connections': [],
            'num_layers': num_layers
        }
        
        current_channels = self.config.min_channels
        
        for i in range(num_layers):
            # Sample operation
            op_name = random.choice(list(self.operations.keys()))
            
            # Sample channels
            if i > 0:
                multiplier = random.choice(self.config.channel_multipliers)
                next_channels = min(current_channels * multiplier, self.config.max_channels)
            else:
                next_channels = current_channels
            
            # Sample activation and normalization
            activation = random.choice(list(self.activations.keys()))
            normalization = random.choice(list(self.normalizations.keys()))
            
            layer_config = {
                'operation': op_name,
                'in_channels': current_channels,
                'out_channels': next_channels,
                'activation': activation,
                'normalization': normalization,
                'layer_index': i
            }
            
            architecture['layers'].append(layer_config)
            architecture['channels'].append(next_channels)
            current_channels = next_channels
        
        # Sample connections (skip connections)
        for i in range(num_layers):
            connections = []
            for j in range(i):
                if random.random() < 0.3:  # 30% chance of skip connection
                    connections.append(j)
            architecture['connections'].append(connections)
        
        return architecture
    
    def mutate_architecture(self, architecture: Dict[str, Any]) -> Dict[str, Any]:
        """Mutate an architecture for evolutionary search"""
        mutated = copy.deepcopy(architecture)
        
        # Mutate operations
        for layer in mutated['layers']:
            if random.random() < self.config.mutation_rate:
                layer['operation'] = random.choice(list(self.operations.keys()))
            
            if random.random() < self.config.mutation_rate:
                layer['activation'] = random.choice(list(self.activations.keys()))
            
            if random.random() < self.config.mutation_rate:
                layer['normalization'] = random.choice(list(self.normalizations.keys()))
        
        # Mutate connections
        for i, connections in enumerate(mutated['connections']):
            if random.random() < self.config.mutation_rate:
                # Add or remove connection
                if connections and random.random() < 0.5:
                    connections.pop(random.randint(0, len(connections) - 1))
                else:
                    possible_connections = [j for j in range(i) if j not in connections]
                    if possible_connections:
                        connections.append(random.choice(possible_connections))
        
        return mutated
    
    def crossover_architectures(self, parent1: Dict[str, Any], parent2: Dict[str, Any]) -> Dict[str, Any]:
        """Crossover two architectures for evolutionary search"""
        child = copy.deepcopy(parent1)
        
        # Crossover layers
        crossover_point = random.randint(1, min(len(parent1['layers']), len(parent2['layers'])) - 1)
        
        for i in range(crossover_point, min(len(parent1['layers']), len(parent2['layers']))):
            if random.random() < self.config.crossover_rate:
                child['layers'][i] = copy.deepcopy(parent2['layers'][i])
                child['channels'][i] = parent2['channels'][i]
                child['connections'][i] = copy.deepcopy(parent2['connections'][i])
        
        return child

class SeparableConv3d(nn.Module):
    """3D Separable convolution"""
    
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int = 0):
        super().__init__()
        self.depthwise = nn.Conv3d(in_channels, in_channels, kernel_size, 
                                 padding=padding, groups=in_channels)
        self.pointwise = nn.Conv3d(in_channels, out_channels, 1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x

class SelfAttention3D(nn.Module):
    """3D Self-attention module"""
    
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        self.query_conv = nn.Conv3d(in_channels, out_channels // 8, 1)
        self.key_conv = nn.Conv3d(in_channels, out_channels // 8, 1)
        self.value_conv = nn.Conv3d(in_channels, out_channels, 1)
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, channels, depth, height, width = x.size()
        
        # Generate query, key, value
        query = self.query_conv(x).view(batch_size, -1, depth * height * width).permute(0, 2, 1)
        key = self.key_conv(x).view(batch_size, -1, depth * height * width)
        value = self.value_conv(x).view(batch_size, -1, depth * height * width)
        
        # Attention
        attention = torch.bmm(query, key)
        attention = self.softmax(attention)
        
        # Apply attention to value
        out = torch.bmm(value, attention.permute(0, 2, 1))
        out = out.view(batch_size, self.out_channels, depth, height, width)
        
        # Residual connection
        out = self.gamma * out + x
        
        return out

class SkipConnection3D(nn.Module):
    """3D Skip connection with optional projection"""
    
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.projection = nn.Conv3d(in_channels, out_channels, 1) if in_channels != out_channels else None
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.projection is not None:
            x = self.projection(x)
        return x

class SuperNet(nn.Module):
    """Super network for differentiable architecture search"""
    
    def __init__(self, search_space: ArchitectureSearchSpace, config: NASConfig):
        super().__init__()
        self.search_space = search_space
        self.config = config
        
        # Architecture parameters (alpha)
        self.num_operations = len(search_space.operations)
        self.num_layers = config.max_layers
        
        # Alpha parameters for operation selection
        self.alphas = nn.Parameter(torch.randn(self.num_layers, self.num_operations))
        
        # Build super network
        self.layers = nn.ModuleList()
        current_channels = config.min_channels
        
        for i in range(self.num_layers):
            # Create mixed operation
            mixed_op = MixedOperation(
                search_space.operations,
                current_channels,
                min(current_channels * 2, config.max_channels)
            )
            self.layers.append(mixed_op)
            current_channels = min(current_channels * 2, config.max_channels)
        
        # Output projection
        self.output_proj = nn.Conv3d(current_channels, config.output_shape[0], 1)
        
        logger.info(f"Initialized SuperNet with {self.num_layers} layers")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through super network"""
        for i, layer in enumerate(self.layers):
            # Get operation weights
            weights = F.softmax(self.alphas[i], dim=0)
            
            # Apply mixed operation
            x = layer(x, weights)
        
        # Output projection
        x = self.output_proj(x)
        return x
    
    def get_architecture(self) -> Dict[str, Any]:
        """Extract discrete architecture from alpha parameters"""
        architecture = {
            'layers': [],
            'channels': [],
            'connections': [],
            'num_layers': self.num_layers
        }
        
        current_channels = self.config.min_channels
        
        for i in range(self.num_layers):
            # Select operation with highest alpha
            op_idx = torch.argmax(self.alphas[i]).item()
            op_name = list(self.search_space.operations.keys())[op_idx]
            
            next_channels = min(current_channels * 2, self.config.max_channels)
            
            layer_config = {
                'operation': op_name,
                'in_channels': current_channels,
                'out_channels': next_channels,
                'activation': 'relu',  # Default
                'normalization': 'batch_norm',  # Default
                'layer_index': i
            }
            
            architecture['layers'].append(layer_config)
            architecture['channels'].append(next_channels)
            current_channels = next_channels
        
        return architecture

class MixedOperation(nn.Module):
    """Mixed operation for differentiable architecture search"""
    
    def __init__(self, operations: Dict[str, Callable], in_channels: int, out_channels: int):
        super().__init__()
        self.operations = nn.ModuleDict()
        
        for op_name, op_func in operations.items():
            self.operations[op_name] = op_func(in_channels, out_channels)
    
    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """Forward pass with weighted operations"""
        output = None
        
        for i, (op_name, op) in enumerate(self.operations.items()):
            op_output = op(x)
            
            if output is None:
                output = weights[i] * op_output
            else:
                output = output + weights[i] * op_output
        
        return output

class DifferentiableArchitectureSearch:
    """DARTS-based differentiable architecture search"""
    
    def __init__(self, search_space: ArchitectureSearchSpace, config: NASConfig):
        self.search_space = search_space
        self.config = config
        
        # Create super network
        self.super_net = SuperNet(search_space, config)
        
        # Optimizers
        self.weight_optimizer = torch.optim.Adam(
            [p for p in self.super_net.parameters() if p is not self.super_net.alphas],
            lr=1e-3
        )
        
        self.arch_optimizer = torch.optim.Adam(
            [self.super_net.alphas],
            lr=3e-4
        )
        
        logger.info("Initialized DARTS architecture search")
    
    def search(self, train_loader, val_loader, num_epochs: int = 50) -> Dict[str, Any]:
        """Perform differentiable architecture search"""
        self.super_net.train()
        
        search_history = []
        
        for epoch in range(num_epochs):
            epoch_metrics = self._train_epoch(train_loader, val_loader)
            search_history.append(epoch_metrics)
            
            logger.info(f"Epoch {epoch+1}/{num_epochs}: "
                       f"Train Loss: {epoch_metrics['train_loss']:.4f}, "
                       f"Val Loss: {epoch_metrics['val_loss']:.4f}")
        
        # Extract final architecture
        final_architecture = self.super_net.get_architecture()
        
        return {
            'architecture': final_architecture,
            'search_history': search_history,
            'alphas': self.super_net.alphas.data.cpu().numpy()
        }
    
    def _train_epoch(self, train_loader, val_loader) -> Dict[str, float]:
        """Train one epoch of architecture search"""
        train_loss = 0.0
        val_loss = 0.0
        
        # Alternate between weight and architecture updates
        for i, (train_batch, val_batch) in enumerate(zip(train_loader, val_loader)):
            train_x, train_y = train_batch
            val_x, val_y = val_batch
            
            # Update weights
            self.weight_optimizer.zero_grad()
            train_pred = self.super_net(train_x)
            train_loss_batch = F.mse_loss(train_pred, train_y)
            train_loss_batch.backward()
            self.weight_optimizer.step()
            
            # Update architecture
            self.arch_optimizer.zero_grad()
            val_pred = self.super_net(val_x)
            val_loss_batch = F.mse_loss(val_pred, val_y)
            val_loss_batch.backward()
            self.arch_optimizer.step()
            
            train_loss += train_loss_batch.item()
            val_loss += val_loss_batch.item()
        
        return {
            'train_loss': train_loss / len(train_loader),
            'val_loss': val_loss / len(val_loader)
        }

class EvolutionaryArchitectureSearch:
    """Evolutionary algorithm for architecture search"""
    
    def __init__(self, search_space: ArchitectureSearchSpace, config: NASConfig):
        self.search_space = search_space
        self.config = config
        self.population = []
        self.fitness_history = []
        
        logger.info("Initialized Evolutionary Architecture Search")
    
    def search(self, fitness_function: Callable, num_generations: int = 50) -> Dict[str, Any]:
        """Perform evolutionary architecture search"""
        # Initialize population
        self.population = [self.search_space.sample_architecture() 
                          for _ in range(self.config.population_size)]
        
        best_architecture = None
        best_fitness = float('-inf')
        
        for generation in range(num_generations):
            # Evaluate population
            fitness_scores = []
            for arch in self.population:
                fitness = fitness_function(arch)
                fitness_scores.append(fitness)
                
                if fitness > best_fitness:
                    best_fitness = fitness
                    best_architecture = copy.deepcopy(arch)
            
            self.fitness_history.append({
                'generation': generation,
                'best_fitness': best_fitness,
                'avg_fitness': np.mean(fitness_scores),
                'std_fitness': np.std(fitness_scores)
            })
            
            # Selection, crossover, and mutation
            new_population = self._evolve_population(fitness_scores)
            self.population = new_population
            
            logger.info(f"Generation {generation+1}/{num_generations}: "
                       f"Best Fitness: {best_fitness:.4f}, "
                       f"Avg Fitness: {np.mean(fitness_scores):.4f}")
        
        return {
            'best_architecture': best_architecture,
            'best_fitness': best_fitness,
            'fitness_history': self.fitness_history
        }
    
    def _evolve_population(self, fitness_scores: List[float]) -> List[Dict[str, Any]]:
        """Evolve population using selection, crossover, and mutation"""
        # Selection (tournament selection)
        selected = self._tournament_selection(fitness_scores)
        
        # Crossover and mutation
        new_population = []
        for i in range(0, len(selected), 2):
            parent1 = selected[i]
            parent2 = selected[i + 1] if i + 1 < len(selected) else selected[0]
            
            # Crossover
            if random.random() < self.config.crossover_rate:
                child = self.search_space.crossover_architectures(parent1, parent2)
            else:
                child = copy.deepcopy(parent1)
            
            # Mutation
            child = self.search_space.mutate_architecture(child)
            new_population.append(child)
        
        return new_population[:self.config.population_size]
    
    def _tournament_selection(self, fitness_scores: List[float]) -> List[Dict[str, Any]]:
        """Tournament selection"""
        selected = []
        
        for _ in range(self.config.population_size):
            # Tournament
            tournament_size = min(3, len(self.population))
            tournament_indices = random.sample(range(len(self.population)), tournament_size)
            
            # Select best from tournament
            best_idx = max(tournament_indices, key=lambda i: fitness_scores[i])
            selected.append(copy.deepcopy(self.population[best_idx]))
        
        return selected

class ArchitectureEvaluator:
    """Evaluator for architecture performance"""
    
    def __init__(self, config: NASConfig):
        self.config = config
        
    def evaluate_architecture(self, architecture: Dict[str, Any], 
                            train_loader, val_loader) -> Dict[str, float]:
        """Evaluate architecture performance"""
        # Build model from architecture
        model = self._build_model(architecture)
        
        # Train model
        train_metrics = self._train_model(model, train_loader, val_loader)
        
        # Evaluate performance
        performance_metrics = self._evaluate_performance(model, val_loader)
        
        # Compute multi-objective score
        multi_objective_score = self._compute_multi_objective_score(performance_metrics)
        
        return {
            'accuracy': performance_metrics['accuracy'],
            'latency': performance_metrics['latency'],
            'memory': performance_metrics['memory'],
            'flops': performance_metrics['flops'],
            'multi_objective_score': multi_objective_score,
            **train_metrics
        }
    
    def _build_model(self, architecture: Dict[str, Any]) -> nn.Module:
        """Build model from architecture description"""
        layers = []
        
        for layer_config in architecture['layers']:
            # Get operation
            op_func = self.config.search_space.operations[layer_config['operation']]
            
            # Create layer
            layer = op_func(layer_config['in_channels'], layer_config['out_channels'])
            
            # Add activation and normalization
            activation = self.config.search_space.activations[layer_config['activation']]
            normalization = self.config.search_space.normalizations[layer_config['normalization']](
                layer_config['out_channels']
            )
            
            layers.extend([layer, normalization, activation])
        
        return nn.Sequential(*layers)
    
    def _train_model(self, model: nn.Module, train_loader, val_loader) -> Dict[str, float]:
        """Train model for evaluation"""
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        
        model.train()
        train_loss = 0.0
        
        # Simple training loop
        for batch_idx, (data, target) in enumerate(train_loader):
            if batch_idx >= 10:  # Limit training for efficiency
                break
                
            optimizer.zero_grad()
            output = model(data)
            loss = F.mse_loss(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        return {'train_loss': train_loss / min(10, len(train_loader))}
    
    def _evaluate_performance(self, model: nn.Module, val_loader) -> Dict[str, float]:
        """Evaluate model performance metrics"""
        model.eval()
        
        # Accuracy
        total_loss = 0.0
        num_samples = 0
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                if batch_idx >= 5:  # Limit evaluation for efficiency
                    break
                    
                output = model(data)
                loss = F.mse_loss(output, target)
                total_loss += loss.item() * data.size(0)
                num_samples += data.size(0)
        
        accuracy = 1.0 / (1.0 + total_loss / num_samples)  # Convert loss to accuracy-like metric
        
        # Latency (simplified)
        latency = self._measure_latency(model)
        
        # Memory (simplified)
        memory = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024  # MB
        
        # FLOPs (simplified)
        flops = sum(p.numel() for p in model.parameters()) * 2  # Rough estimate
        
        return {
            'accuracy': accuracy,
            'latency': latency,
            'memory': memory,
            'flops': flops
        }
    
    def _measure_latency(self, model: nn.Module) -> float:
        """Measure model latency"""
        dummy_input = torch.randn(1, *self.config.input_shape)
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = model(dummy_input)
        
        # Measure
        import time
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(100):
                _ = model(dummy_input)
        
        end_time = time.time()
        latency = (end_time - start_time) / 100 * 1000  # ms
        
        return latency
    
    def _compute_multi_objective_score(self, metrics: Dict[str, float]) -> float:
        """Compute multi-objective score"""
        score = 0.0
        
        for i, objective in enumerate(self.config.objectives):
            weight = self.config.objective_weights[i]
            
            if objective == 'accuracy':
                score += weight * metrics['accuracy']
            elif objective == 'latency':
                # Minimize latency
                score += weight * (1.0 / (1.0 + metrics['latency'] / 100.0))
            elif objective == 'memory':
                # Minimize memory
                score += weight * (1.0 / (1.0 + metrics['memory'] / 1000.0))
            elif objective == 'flops':
                # Minimize FLOPs
                score += weight * (1.0 / (1.0 + metrics['flops'] / 1000000.0))
        
        return score

class NeuralArchitectureSearchOrchestrator:
    """Orchestrator for Neural Architecture Search"""
    
    def __init__(self, config: NASConfig):
        self.config = config
        self.search_space = ArchitectureSearchSpace(config)
        self.evaluator = ArchitectureEvaluator(config)
        self.search_history = []
        
        logger.info("Initialized Neural Architecture Search Orchestrator")
    
    def search(self, train_loader, val_loader) -> Dict[str, Any]:
        """Perform neural architecture search"""
        if self.config.search_algorithm == "darts":
            searcher = DifferentiableArchitectureSearch(self.search_space, self.config)
            results = searcher.search(train_loader, val_loader, self.config.max_epochs)
            
        elif self.config.search_algorithm == "evolutionary":
            searcher = EvolutionaryArchitectureSearch(self.search_space, self.config)
            
            # Define fitness function
            def fitness_function(architecture):
                metrics = self.evaluator.evaluate_architecture(architecture, train_loader, val_loader)
                return metrics['multi_objective_score']
            
            results = searcher.search(fitness_function, self.config.max_epochs)
            
        else:
            raise ValueError(f"Unknown search algorithm: {self.config.search_algorithm}")
        
        # Store search history
        self.search_history.append(results)
        
        return results
    
    def get_best_architecture(self) -> Optional[Dict[str, Any]]:
        """Get the best architecture found so far"""
        if not self.search_history:
            return None
        
        best_result = max(self.search_history, key=lambda x: x.get('best_fitness', 0))
        return best_result.get('best_architecture') or best_result.get('architecture')

# Global NAS orchestrator
_nas_orchestrator = None

def get_nas_orchestrator(config: Optional[NASConfig] = None) -> NeuralArchitectureSearchOrchestrator:
    """Get global NAS orchestrator"""
    global _nas_orchestrator
    if _nas_orchestrator is None:
        if config is None:
            config = NASConfig()
        _nas_orchestrator = NeuralArchitectureSearchOrchestrator(config)
    return _nas_orchestrator

def create_nas_config(task_type: str = "climate_modeling", 
                     search_algorithm: str = "darts",
                     max_epochs: int = 50) -> NASConfig:
    """Create NAS configuration for specific task"""
    config = NASConfig(
        task_type=task_type,
        search_algorithm=search_algorithm,
        max_epochs=max_epochs
    )
    
    # Task-specific configurations
    if task_type == "climate_modeling":
        config.input_shape = (5, 32, 64, 64)
        config.output_shape = (5, 32, 64, 64)
        config.objectives = ["accuracy", "latency"]
        config.objective_weights = [0.8, 0.2]
        
    elif task_type == "atmospheric_dynamics":
        config.input_shape = (3, 16, 32, 32)
        config.output_shape = (3, 16, 32, 32)
        config.objectives = ["accuracy", "memory"]
        config.objective_weights = [0.7, 0.3]
        
    elif task_type == "biology":
        config.input_shape = (10, 8, 16, 16)
        config.output_shape = (1, 8, 16, 16)
        config.objectives = ["accuracy", "flops"]
        config.objective_weights = [0.9, 0.1]
    
    return config

if __name__ == "__main__":
    # Demonstration of NAS capabilities
    print("[SEARCH] Neural Architecture Search Demonstration")
    print("=" * 50)
    
    # Create NAS configuration
    config = create_nas_config("climate_modeling", "darts", 10)
    
    # Create NAS orchestrator
    orchestrator = get_nas_orchestrator(config)
    
    print(f"[OK] Created NAS orchestrator for {config.task_type}")
    print(f"[DATA] Search algorithm: {config.search_algorithm}")
    print(f"[TARGET] Objectives: {config.objectives}")
    print(f"üèóÔ∏è Search space: {len(orchestrator.search_space.operations)} operations")
    
    # Sample architecture
    sample_arch = orchestrator.search_space.sample_architecture()
    print(f"[BOARD] Sample architecture: {sample_arch['num_layers']} layers")
    
    print("[START] Neural Architecture Search ready for optimal model discovery!")
    print("[AI] Ready to discover world-class architectures for astrobiology research!") 