{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import platform\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch==2.8.0\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu126\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch-geometric\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyg_lib\", \"torch_scatter\", \"torch_sparse\", \"torch_cluster\", \"torch_spline_conv\", \"-f\", \"https://data.pyg.org/whl/torch-2.8.0+cu126.html\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"flash-attn\", \"--no-build-isolation\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xformers\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"triton\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"accelerate\", \"datasets\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"wandb\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"astropy\", \"astroquery\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"biopython\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xarray\", \"zarr\", \"netCDF4\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"h5py\", \"tables\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scipy\", \"scikit-learn\", \"pandas\", \"numpy\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"matplotlib\", \"seaborn\", \"plotly\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tqdm\", \"rich\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyyaml\", \"omegaconf\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"einops\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm\"], check=True)\n",
    "print(\"All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"Flash Attention: {flash_attn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Flash Attention: Not available\")\n",
    "try:\n",
    "    import xformers\n",
    "    print(f\"xFormers: {xformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"xFormers: Not available\")\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"bitsandbytes: {bnb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"bitsandbytes: Not available\")\n",
    "try:\n",
    "    import torch_geometric\n",
    "    print(f\"PyTorch Geometric: {torch_geometric.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric: Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "try:\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp import CPUOffload\n",
    "    FSDP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FSDP_AVAILABLE = False\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    BITSANDBYTES_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BITSANDBYTES_AVAILABLE = False\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/workspace/training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "os.environ['WORLD_SIZE'] = '2'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_MAST_TOKEN = '54f271a4785a4ae19ffa5d0aff35c36c'\n",
    "CLIMATE_DATA_STORE_KEY = '4dc6dcb0-c145-476f-baf9-d10eb524fb20'\n",
    "NCBI_API_KEY = '64e1952dfbdd9791d8ec9b18ae2559ec0e09'\n",
    "ESA_GAIA_USERNAME = 'sjiang02'\n",
    "ESO_USERNAME = 'Shengboj324'\n",
    "os.environ['MAST_API_TOKEN'] = NASA_MAST_TOKEN\n",
    "os.environ['CDS_API_KEY'] = CLIMATE_DATA_STORE_KEY\n",
    "os.environ['NCBI_API_KEY'] = NCBI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_DIR = Path('/workspace')\n",
    "DATA_DIR = WORKSPACE_DIR / 'data'\n",
    "CHECKPOINT_DIR = WORKSPACE_DIR / 'checkpoints'\n",
    "LOG_DIR = WORKSPACE_DIR / 'logs'\n",
    "OUTPUT_DIR = WORKSPACE_DIR / 'outputs'\n",
    "for dir_path in [DATA_DIR, CHECKPOINT_DIR, LOG_DIR, OUTPUT_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Workspace directories created: {WORKSPACE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(WORKSPACE_DIR))\n",
    "from models.rebuilt_llm_integration import RebuiltLLMIntegration\n",
    "from models.rebuilt_graph_vae import RebuiltGraphVAE\n",
    "from models.rebuilt_datacube_cnn import RebuiltDatacubeCNN\n",
    "from models.rebuilt_multimodal_integration import RebuiltMultimodalIntegration\n",
    "from training.unified_multimodal_training import UnifiedMultiModalSystem, MultiModalTrainingConfig, compute_multimodal_loss\n",
    "from data_build.unified_dataloader_architecture import MultiModalBatch, multimodal_collate_fn, DataLoaderConfig\n",
    "from data_build.comprehensive_data_annotation_treatment import ComprehensiveDataAnnotationSystem, DataDomain, TreatmentConfig\n",
    "from data_build.source_domain_mapping import get_source_domain_mapper  # NEW: 1000+ sources support\n",
    "logger.info(\"All model modules imported successfully (with 1000+ sources annotation support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunPodTrainingConfig:\n",
    "    num_gpus: int = 2\n",
    "    gpu_type: str = \"RTX_A5000\"\n",
    "    total_vram_gb: int = 48\n",
    "    max_epochs: int = 200\n",
    "    batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 32\n",
    "    effective_batch_size: int = 32\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    warmup_steps: int = 1000\n",
    "    use_mixed_precision: bool = True\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    use_8bit_optimizer: bool = True\n",
    "    use_cpu_offloading: bool = True\n",
    "    use_flash_attention: bool = True\n",
    "    use_distributed: bool = True\n",
    "    distributed_backend: str = \"nccl\"\n",
    "    save_every_n_epochs: int = 10\n",
    "    checkpoint_every_hours: int = 2\n",
    "    log_every_n_steps: int = 50\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"astrobiology-ai-training\"\n",
    "    wandb_entity: Optional[str] = None\n",
    "    max_grad_norm: float = 1.0\n",
    "    early_stopping_patience: int = 20\n",
    "    target_accuracy: float = 0.96\n",
    "    max_training_hours: int = 672\n",
    "config = RunPodTrainingConfig()\n",
    "logger.info(f\"Training configuration initialized: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_distributed(rank, world_size):\n",
    "    os.environ['RANK'] = str(rank)\n",
    "    os.environ['LOCAL_RANK'] = str(rank)\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\n",
    "            backend=config.distributed_backend,\n",
    "            init_method='env://',\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            timeout=timedelta(minutes=30)\n",
    "        )\n",
    "    torch.cuda.set_device(rank)\n",
    "    logger.info(f\"Distributed training initialized: rank={rank}, world_size={world_size}\")\n",
    "def cleanup_distributed():\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "        logger.info(\"Distributed training cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMonitor:\n",
    "    def __init__(self, log_interval=30):\n",
    "        self.log_interval = log_interval\n",
    "        self.last_log_time = time.time()\n",
    "        self.stats_history = []\n",
    "    def get_gpu_stats(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            return {}\n",
    "        stats = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            total = props.total_memory / 1024**3\n",
    "            stats[f'gpu_{i}'] = {\n",
    "                'allocated_gb': allocated,\n",
    "                'reserved_gb': reserved,\n",
    "                'total_gb': total,\n",
    "                'utilization_pct': (allocated / total) * 100\n",
    "            }\n",
    "        return stats\n",
    "    def log_stats(self, force=False):\n",
    "        current_time = time.time()\n",
    "        if force or (current_time - self.last_log_time) >= self.log_interval:\n",
    "            stats = self.get_gpu_stats()\n",
    "            self.stats_history.append({'timestamp': current_time, 'stats': stats})\n",
    "            for gpu_id, gpu_stats in stats.items():\n",
    "                logger.info(f\"{gpu_id}: {gpu_stats['allocated_gb']:.2f}GB / {gpu_stats['total_gb']:.2f}GB ({gpu_stats['utilization_pct']:.1f}%)\")\n",
    "            self.last_log_time = current_time\n",
    "            return stats\n",
    "        return None\n",
    "gpu_monitor = GPUMonitor(log_interval=30)\n",
    "logger.info(\"GPU monitor initialized\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MockMultiModalDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, split='train'):\n",
    "        self.num_samples = num_samples\n",
    "        self.split = split\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        np.random.seed(idx)\n",
    "        climate_cube = torch.randn(2, 12, 32, 64, 10)\n",
    "        from torch_geometric.data import Data as PyGData\n",
    "        num_nodes = 50\n",
    "        num_edges = 100\n",
    "        edge_index = torch.randint(0, num_nodes, (2, num_edges))\n",
    "        node_features = torch.randn(num_nodes, 64)\n",
    "        bio_graph = PyGData(x=node_features, edge_index=edge_index, num_nodes=num_nodes)\n",
    "        spectroscopy = torch.randn(1000, 3)\n",
    "        text_description = f\"Exoplanet {idx}: habitability assessment\"\n",
    "        habitability_label = torch.randint(0, 2, (1,)).item()\n",
    "        return {\n",
    "            'run_id': idx,\n",
    "            'planet_params': torch.randn(10),\n",
    "            'climate_cube': climate_cube,\n",
    "            'bio_graph': bio_graph,\n",
    "            'spectrum': spectroscopy,\n",
    "            'text_description': text_description,\n",
    "            'habitability_label': habitability_label,\n",
    "            'metadata': {'split': self.split}\n",
    "        }\n",
    "logger.info(\"Mock dataset class defined\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def custom_collate_fn(batch):\n",
    "    from torch_geometric.data import Batch as PyGBatch\n",
    "    run_ids = torch.tensor([item['run_id'] for item in batch], dtype=torch.long)\n",
    "    planet_params = torch.stack([item['planet_params'] for item in batch])\n",
    "    climate_cubes = torch.stack([item['climate_cube'] for item in batch])\n",
    "    bio_graphs = PyGBatch.from_data_list([item['bio_graph'] for item in batch])\n",
    "    spectra = torch.stack([item['spectrum'] for item in batch])\n",
    "    text_descriptions = [item['text_description'] for item in batch]\n",
    "    habitability_labels = torch.tensor([item['habitability_label'] for item in batch], dtype=torch.long)\n",
    "    metadata = [item['metadata'] for item in batch]\n",
    "    return {\n",
    "        'run_ids': run_ids,\n",
    "        'planet_params': planet_params,\n",
    "        'climate_datacube': climate_cubes,\n",
    "        'metabolic_graph': bio_graphs,\n",
    "        'spectroscopy': spectra,\n",
    "        'text_description': text_descriptions,\n",
    "        'habitability_label': habitability_labels,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "logger.info(\"Custom collate function defined\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = MockMultiModalDataset(num_samples=8000, split='train')\n",
    "val_dataset = MockMultiModalDataset(num_samples=1000, split='val')\n",
    "test_dataset = MockMultiModalDataset(num_samples=1000, split='test')\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "logger.info(f\"Data loaders created: train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "annotation_system = ComprehensiveDataAnnotationSystem()\n",
    "treatment_config = TreatmentConfig()\n",
    "logger.info(\"âœ… Comprehensive data annotation system initialized (supports 14 domains)\")\n",
    "annotation_stats = annotation_system.get_statistics()\n",
    "logger.info(f\"ðŸ“Š Annotation statistics: {annotation_stats}\")\n",
    "\n",
    "# Initialize source mapper for 1000+ sources\n",
    "source_mapper = get_source_domain_mapper()\n",
    "mapper_stats = source_mapper.get_statistics()\n",
    "logger.info(f\"âœ… Source mapper initialized: {mapper_stats['total_sources']} sources across {len(mapper_stats['by_domain'])} domains\")\n",
    "logger.info(f\"ðŸ“Š Sources by domain: {mapper_stats['by_domain']}\")\n",
    "logger.info(f\"ðŸ“Š Average quality score: {mapper_stats['avg_quality_score']:.3f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "multimodal_config = MultiModalTrainingConfig(\n",
    "    llm_config={\n",
    "        'hidden_size': 4352,\n",
    "        'num_attention_heads': 64,\n",
    "        'use_4bit_quantization': False,\n",
    "        'use_lora': False,\n",
    "        'use_scientific_reasoning': True,\n",
    "        'use_rope': True,\n",
    "        'use_gqa': True,\n",
    "        'use_rms_norm': True,\n",
    "        'use_swiglu': True\n",
    "    },\n",
    "    graph_config={\n",
    "        'node_features': 64,\n",
    "        'hidden_dim': 512,\n",
    "        'latent_dim': 256,\n",
    "        'max_nodes': 50,\n",
    "        'num_layers': 6,\n",
    "        'heads': 16,\n",
    "        'use_biochemical_constraints': True\n",
    "    },\n",
    "    cnn_config={\n",
    "        'input_variables': 2,\n",
    "        'output_variables': 2,\n",
    "        'base_channels': 128,\n",
    "        'depth': 6,\n",
    "        'use_attention': True,\n",
    "        'use_physics_constraints': True,\n",
    "        'embed_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_transformer_layers': 6,\n",
    "        'use_vit_features': True,\n",
    "        'use_gradient_checkpointing': True\n",
    "    },\n",
    "    fusion_config={\n",
    "        'fusion_dim': 1024,\n",
    "        'num_attention_heads': 8,\n",
    "        'num_fusion_layers': 3,\n",
    "        'use_adaptive_weighting': True\n",
    "    },\n",
    "    classification_weight=1.0,\n",
    "    reconstruction_weight=0.1,\n",
    "    physics_weight=0.2,\n",
    "    consistency_weight=0.15,\n",
    "    batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    use_gradient_checkpointing=config.use_gradient_checkpointing,\n",
    "    use_mixed_precision=config.use_mixed_precision,\n",
    "    use_8bit_optimizer=config.use_8bit_optimizer,\n",
    "    device='cuda'\n",
    ")\n",
    "logger.info(\"Multi-modal training configuration created\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = UnifiedMultiModalSystem(multimodal_config)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Model initialized: {total_params/1e9:.2f}B total params, {trainable_params/1e9:.2f}B trainable\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "logger.info(f\"Model moved to device: {device}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if config.use_gradient_checkpointing:\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        logger.info(\"Gradient checkpointing enabled\")\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'gradient_checkpointing'):\n",
    "            module.gradient_checkpointing = True\n",
    "        if hasattr(module, 'use_checkpoint'):\n",
    "            module.use_checkpoint = True"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if config.use_8bit_optimizer and BITSANDBYTES_AVAILABLE:\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    logger.info(\"8-bit AdamW optimizer initialized (75% memory reduction)\")\n",
    "else:\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    logger.info(\"Standard AdamW optimizer initialized\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_steps = len(train_loader) * config.max_epochs // config.gradient_accumulation_steps\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.learning_rate * 10,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10.0,\n",
    "    final_div_factor=100.0\n",
    ")\n",
    "logger.info(f\"OneCycleLR scheduler initialized: {total_steps} total steps\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = GradScaler() if config.use_mixed_precision else None\n",
    "if scaler:\n",
    "    logger.info(\"Mixed precision scaler initialized\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if config.use_wandb and WANDB_AVAILABLE:\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        entity=config.wandb_entity,\n",
    "        config={\n",
    "            'num_gpus': config.num_gpus,\n",
    "            'gpu_type': config.gpu_type,\n",
    "            'total_vram_gb': config.total_vram_gb,\n",
    "            'max_epochs': config.max_epochs,\n",
    "            'batch_size': config.batch_size,\n",
    "            'gradient_accumulation_steps': config.gradient_accumulation_steps,\n",
    "            'effective_batch_size': config.effective_batch_size,\n",
    "            'learning_rate': config.learning_rate,\n",
    "            'weight_decay': config.weight_decay,\n",
    "            'use_mixed_precision': config.use_mixed_precision,\n",
    "            'use_gradient_checkpointing': config.use_gradient_checkpointing,\n",
    "            'use_8bit_optimizer': config.use_8bit_optimizer,\n",
    "            'use_cpu_offloading': config.use_cpu_offloading,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        },\n",
    "        name=f\"astrobio_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    logger.info(\"Weights & Biases initialized\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scheduler, scaler, metrics, is_best=False):\n",
    "    checkpoint_path = CHECKPOINT_DIR / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "    best_path = CHECKPOINT_DIR / \"best_model.pt\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "        'metrics': metrics,\n",
    "        'config': config,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    if is_best:\n",
    "        torch.save(checkpoint, best_path)\n",
    "        logger.info(f\"Best model saved: {best_path}\")\n",
    "    old_checkpoints = sorted(CHECKPOINT_DIR.glob(\"checkpoint_epoch_*.pt\"))\n",
    "    if len(old_checkpoints) > 5:\n",
    "        for old_ckpt in old_checkpoints[:-5]:\n",
    "            old_ckpt.unlink()\n",
    "            logger.info(f\"Removed old checkpoint: {old_ckpt}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scheduler, scaler, device, config):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_metrics = {'classification': 0.0, 'llm': 0.0, 'graph_vae': 0.0, 'total': 0.0}\n",
    "    num_batches = len(train_loader)\n",
    "    accumulation_steps = config.gradient_accumulation_steps\n",
    "    optimizer.zero_grad()\n",
    "    start_time = time.time()\n",
    "    last_checkpoint_time = start_time\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device)\n",
    "            elif hasattr(batch[key], 'to'):\n",
    "                batch[key] = batch[key].to(device)\n",
    "        if config.use_mixed_precision and scaler is not None:\n",
    "            with autocast():\n",
    "                outputs = model(batch)\n",
    "                total_loss, loss_dict = compute_multimodal_loss(outputs, batch, model.config)\n",
    "        else:\n",
    "            outputs = model(batch)\n",
    "            total_loss, loss_dict = compute_multimodal_loss(outputs, batch, model.config)\n",
    "        loss = total_loss / accumulation_steps\n",
    "        if config.use_mixed_precision and scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == num_batches:\n",
    "            if config.use_mixed_precision and scaler is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        for key in loss_dict:\n",
    "            if key in epoch_metrics:\n",
    "                epoch_metrics[key] += loss_dict[key]\n",
    "        if (batch_idx + 1) % config.log_every_n_steps == 0:\n",
    "            avg_loss = epoch_loss / (batch_idx + 1)\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            logger.info(f\"Epoch {epoch} [{batch_idx+1}/{num_batches}] Loss: {avg_loss:.4f} LR: {lr:.2e}\")\n",
    "            if config.use_wandb and WANDB_AVAILABLE:\n",
    "                wandb.log({\n",
    "                    'train/loss': avg_loss,\n",
    "                    'train/lr': lr,\n",
    "                    'train/epoch': epoch,\n",
    "                    'train/step': epoch * num_batches + batch_idx\n",
    "                })\n",
    "        current_time = time.time()\n",
    "        if (current_time - last_checkpoint_time) >= (config.checkpoint_every_hours * 3600):\n",
    "            save_checkpoint(epoch, model, optimizer, scheduler, scaler, {'loss': epoch_loss / (batch_idx + 1)}, is_best=False)\n",
    "            last_checkpoint_time = current_time\n",
    "        gpu_monitor.log_stats()\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    for key in epoch_metrics:\n",
    "        epoch_metrics[key] /= num_batches\n",
    "    logger.info(f\"Epoch {epoch} completed in {epoch_time:.2f}s - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    return avg_epoch_loss, epoch_metrics"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate_epoch(epoch, model, val_loader, device, config):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'classification': 0.0, 'llm': 0.0, 'graph_vae': 0.0, 'total': 0.0}\n",
    "    num_batches = len(val_loader)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            for key in batch:\n",
    "                if isinstance(batch[key], torch.Tensor):\n",
    "                    batch[key] = batch[key].to(device)\n",
    "                elif hasattr(batch[key], 'to'):\n",
    "                    batch[key] = batch[key].to(device)\n",
    "            if config.use_mixed_precision:\n",
    "                with autocast():\n",
    "                    outputs = model(batch)\n",
    "                    total_loss, loss_dict = compute_multimodal_loss(outputs, batch, model.config)\n",
    "            else:\n",
    "                outputs = model(batch)\n",
    "                total_loss, loss_dict = compute_multimodal_loss(outputs, batch, model.config)\n",
    "            val_loss += total_loss.item()\n",
    "            for key in loss_dict:\n",
    "                if key in val_metrics:\n",
    "                    val_metrics[key] += loss_dict[key]\n",
    "            if 'logits' in outputs and 'habitability_label' in batch:\n",
    "                predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "                correct += (predictions == batch['habitability_label']).sum().item()\n",
    "                total += batch['habitability_label'].size(0)\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    for key in val_metrics:\n",
    "        val_metrics[key] /= num_batches\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    logger.info(f\"Validation Epoch {epoch} - Loss: {avg_val_loss:.4f} Accuracy: {accuracy:.4f}\")\n",
    "    if config.use_wandb and WANDB_AVAILABLE:\n",
    "        wandb.log({\n",
    "            'val/loss': avg_val_loss,\n",
    "            'val/accuracy': accuracy,\n",
    "            'val/epoch': epoch\n",
    "        })\n",
    "    return avg_val_loss, accuracy, val_metrics"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "training_start_time = time.time()\n",
    "training_history = []\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"STARTING TRAINING\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Configuration:\")\n",
    "logger.info(f\"  GPUs: {config.num_gpus} x {config.gpu_type}\")\n",
    "logger.info(f\"  Total VRAM: {config.total_vram_gb} GB\")\n",
    "logger.info(f\"  Max Epochs: {config.max_epochs}\")\n",
    "logger.info(f\"  Batch Size: {config.batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation: {config.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Effective Batch Size: {config.effective_batch_size}\")\n",
    "logger.info(f\"  Learning Rate: {config.learning_rate}\")\n",
    "logger.info(f\"  Mixed Precision: {config.use_mixed_precision}\")\n",
    "logger.info(f\"  Gradient Checkpointing: {config.use_gradient_checkpointing}\")\n",
    "logger.info(f\"  8-bit Optimizer: {config.use_8bit_optimizer}\")\n",
    "logger.info(f\"  CPU Offloading: {config.use_cpu_offloading}\")\n",
    "logger.info(f\"  Model Parameters: {total_params/1e9:.2f}B\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for epoch in range(1, config.max_epochs + 1):\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"EPOCH {epoch}/{config.max_epochs}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        epoch, model, train_loader, optimizer, scheduler, scaler, device, config\n",
    "    )\n",
    "    val_loss, val_accuracy, val_metrics = validate_epoch(\n",
    "        epoch, model, val_loader, device, config\n",
    "    )\n",
    "    epoch_history = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_metrics': train_metrics,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_metrics': val_metrics,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    training_history.append(epoch_history)\n",
    "    is_best = False\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        is_best = True\n",
    "        logger.info(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        logger.info(f\"No improvement. Patience: {patience_counter}/{config.early_stopping_patience}\")\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        logger.info(f\"New best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    if epoch % config.save_every_n_epochs == 0 or is_best:\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, scaler, epoch_history, is_best=is_best)\n",
    "    if patience_counter >= config.early_stopping_patience:\n",
    "        logger.info(f\"Early stopping triggered after {epoch} epochs\")\n",
    "        break\n",
    "    if val_accuracy >= config.target_accuracy:\n",
    "        logger.info(f\"Target accuracy {config.target_accuracy:.2%} reached!\")\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, scaler, epoch_history, is_best=True)\n",
    "        break\n",
    "    elapsed_hours = (time.time() - training_start_time) / 3600\n",
    "    if elapsed_hours >= config.max_training_hours:\n",
    "        logger.info(f\"Maximum training time {config.max_training_hours} hours reached\")\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, scaler, epoch_history, is_best=False)\n",
    "        break\n",
    "    gpu_monitor.log_stats(force=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_training_time = time.time() - training_start_time\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"TRAINING COMPLETED\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Total Training Time: {total_training_time/3600:.2f} hours\")\n",
    "logger.info(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "logger.info(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "logger.info(f\"Total Epochs: {len(training_history)}\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history_path = LOG_DIR / f\"training_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "logger.info(f\"Training history saved: {history_path}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if config.use_wandb and WANDB_AVAILABLE:\n",
    "    wandb.log({\n",
    "        'final/best_val_loss': best_val_loss,\n",
    "        'final/best_val_accuracy': best_val_accuracy,\n",
    "        'final/total_epochs': len(training_history),\n",
    "        'final/total_training_hours': total_training_time / 3600\n",
    "    })\n",
    "    wandb.finish()\n",
    "    logger.info(\"Weights & Biases run finished\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logger.info(\"Testing best model on test set...\")\n",
    "best_checkpoint = torch.load(CHECKPOINT_DIR / \"best_model.pt\")\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device)\n",
    "            elif hasattr(batch[key], 'to'):\n",
    "                batch[key] = batch[key].to(device)\n",
    "        outputs = model(batch)\n",
    "        total_loss, loss_dict = compute_multimodal_loss(outputs, batch, model.config)\n",
    "        test_loss += total_loss.item()\n",
    "        if 'logits' in outputs and 'habitability_label' in batch:\n",
    "            predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "            test_correct += (predictions == batch['habitability_label']).sum().item()\n",
    "            test_total += batch['habitability_label'].size(0)\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = test_correct / test_total if test_total > 0 else 0.0\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"TEST SET RESULTS\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "logger.info(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_stats = gpu_monitor.get_gpu_stats()\n",
    "logger.info(\"Final GPU Statistics:\")\n",
    "for gpu_id, stats in final_stats.items():\n",
    "    logger.info(f\"{gpu_id}: {stats['allocated_gb']:.2f}GB / {stats['total_gb']:.2f}GB ({stats['utilization_pct']:.1f}%)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logger.info(\"Training notebook execution completed successfully!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
