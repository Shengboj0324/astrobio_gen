{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunPod Deployment & Validation Notebook\n",
    "## Astrobiology AI Platform - Production Training Setup\n",
    "\n",
    "**Date:** 2025-10-06  \n",
    "**Environment:** RunPod with 2x Nvidia RTX A5000 GPUs (48GB total VRAM)  \n",
    "**Model:** 13.14B parameter multi-modal AI platform  \n",
    "**Training Duration:** 4 weeks  \n",
    "**Target Accuracy:** 96%+\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Environment Setup & Validation\n",
    "2. Dependency Installation\n",
    "3. GPU Configuration & Testing\n",
    "4. Memory Optimization Tests\n",
    "5. Production Readiness Tests\n",
    "6. 100-Step Training Validation\n",
    "7. Production Training Launch\n",
    "8. Monitoring & Checkpointing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system information\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PYTORCH & CUDA INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify we have 2 A5000 GPUs\n",
    "assert torch.cuda.device_count() == 2, \"Expected 2 GPUs\"\n",
    "print(\"\\n✅ GPU Configuration Validated: 2 GPUs available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install critical dependencies\n",
    "print(\"Installing bitsandbytes (8-bit optimizer)...\")\n",
    "!pip install bitsandbytes\n",
    "\n",
    "print(\"\\nInstalling flash-attn (Linux only)...\")\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "print(\"\\nInstalling torch_geometric...\")\n",
    "!pip install torch_geometric\n",
    "\n",
    "print(\"\\n✅ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "print(\"=\"*70)\n",
    "print(\"DEPENDENCY VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(\"✅ bitsandbytes imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ bitsandbytes import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    print(\"✅ flash-attn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ flash-attn import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    print(\"✅ torch_geometric imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ torch_geometric import failed: {e}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Configuration & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure distributed training environment\n",
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['WORLD_SIZE'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "print(\"✅ Distributed training environment configured\")\n",
    "print(f\"   MASTER_ADDR: {os.environ['MASTER_ADDR']}\")\n",
    "print(f\"   MASTER_PORT: {os.environ['MASTER_PORT']}\")\n",
    "print(f\"   WORLD_SIZE: {os.environ['WORLD_SIZE']}\")\n",
    "print(f\"   CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU memory allocation\n",
    "import torch\n",
    "\n",
    "print(\"Testing GPU memory allocation...\")\n",
    "\n",
    "for gpu_id in range(torch.cuda.device_count()):\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Allocate test tensor\n",
    "    test_tensor = torch.randn(1000, 1000, device=f'cuda:{gpu_id}')\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated(gpu_id) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(gpu_id) / 1e9\n",
    "    \n",
    "    print(f\"\\nGPU {gpu_id}:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    del test_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n✅ GPU memory allocation test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Optimization Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run memory optimization tests\n",
    "import sys\n",
    "sys.path.insert(0, '/workspace/astrobio_gen')\n",
    "\n",
    "print(\"Running memory optimization tests...\")\n",
    "!cd /workspace/astrobio_gen && python tests/test_memory_optimizations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Readiness Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run production readiness tests\n",
    "print(\"Running production readiness tests...\")\n",
    "!cd /workspace/astrobio_gen && python -m pytest tests/test_production_readiness.py -v -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Training Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup W&B logging\n",
    "import wandb\n",
    "\n",
    "print(\"Setting up Weights & Biases...\")\n",
    "wandb.login()\n",
    "\n",
    "print(\"\\n✅ W&B configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch production training\n",
    "print(\"=\"*70)\n",
    "print(\"LAUNCHING PRODUCTION TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Model: RebuiltLLMIntegration (13.14B parameters)\")\n",
    "print(\"GPUs: 2x Nvidia RTX A5000 (48GB total VRAM)\")\n",
    "print(\"Duration: 4 weeks\")\n",
    "print(\"Target Accuracy: 96%+\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!cd /workspace/astrobio_gen && python train_unified_sota.py \\\n",
    "    --model rebuilt_llm_integration \\\n",
    "    --epochs 100 \\\n",
    "    --batch-size 32 \\\n",
    "    --micro-batch-size 1 \\\n",
    "    --gradient-accumulation-steps 32 \\\n",
    "    --use-8bit-optimizer \\\n",
    "    --use-cpu-offloading \\\n",
    "    --use-mixed-precision \\\n",
    "    --use-gradient-checkpointing \\\n",
    "    --distributed \\\n",
    "    --gpus 2 \\\n",
    "    --log-every-n-steps 10 \\\n",
    "    --save-every-n-epochs 1 \\\n",
    "    --output-dir outputs/production_training \\\n",
    "    --wandb-project astrobiology-ai-platform \\\n",
    "    --wandb-name production-training-4week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitoring & Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor training progress\n",
    "import glob\n",
    "import os\n",
    "\n",
    "checkpoint_dir = '/workspace/astrobio_gen/outputs/production_training/checkpoints'\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = sorted(glob.glob(f\"{checkpoint_dir}/*.pt\"))\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for ckpt in checkpoints[-5:]:  # Show last 5\n",
    "        size_mb = os.path.getsize(ckpt) / 1e6\n",
    "        print(f\"  {os.path.basename(ckpt)}: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage during training\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CURRENT GPU MEMORY USAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for gpu_id in range(torch.cuda.device_count()):\n",
    "    allocated = torch.cuda.memory_allocated(gpu_id) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(gpu_id) / 1e9\n",
    "    max_allocated = torch.cuda.max_memory_allocated(gpu_id) / 1e9\n",
    "    \n",
    "    print(f\"\\nGPU {gpu_id}:\")\n",
    "    print(f\"  Current Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Current Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Peak Allocated: {max_allocated:.2f} GB\")\n",
    "    print(f\"  Target: <45 GB\")\n",
    "    \n",
    "    if allocated > 45:\n",
    "        print(f\"  ⚠️ WARNING: Memory usage exceeds target!\")\n",
    "    else:\n",
    "        print(f\"  ✅ Memory usage within target\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

