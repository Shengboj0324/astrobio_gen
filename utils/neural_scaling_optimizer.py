#!/usr/bin/env python3
"""
Neural Scaling Laws Optimizer
============================

Advanced system for optimizing neural network architectures using scaling laws principles.
Implements Chinchilla, PaLM, and GPT scaling laws for optimal compute efficiency.

Features:
- Chinchilla scaling laws for compute-optimal training
- PaLM scaling laws for parameter efficiency
- Dynamic architecture optimization
- Multi-objective optimization (accuracy, speed, memory)
- Real-time performance prediction
- Automated hyperparameter tuning
- Resource constraint optimization

Key Scaling Laws:
- Chinchilla: N âˆ C^0.5 (optimal model size scales with square root of compute)
- PaLM: Performance âˆ (N^a * D^b * C^c) where N=parameters, D=data, C=compute
- Kaplan: Loss âˆ (N^-Î± * D^-Î² * C^-Î³)
"""

import math
import logging
import numpy as np
import torch
import json
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize, differential_evolution
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, Matern
import optuna
from concurrent.futures import ThreadPoolExecutor
import asyncio

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ComputeBudget:
    """Compute budget specification"""
    total_flops: float  # Total FLOPs available
    training_flops: float  # FLOPs for training
    inference_flops: float  # FLOPs for inference
    memory_gb: float  # Available memory in GB
    time_hours: float  # Time budget in hours
    gpu_count: int = 1
    gpu_type: str = "A100"
    
    def __post_init__(self):
        """Validate budget constraints"""
        if self.training_flops + self.inference_flops > self.total_flops:
            logger.warning("Training + inference FLOPs exceed total budget")

@dataclass
class DataBudget:
    """Data budget specification"""
    total_tokens: int  # Total training tokens
    high_quality_tokens: int  # High-quality tokens
    synthetic_tokens: int  # Synthetic/augmented tokens
    validation_tokens: int  # Validation tokens
    domains: List[str] = field(default_factory=list)  # Data domains
    
    def __post_init__(self):
        """Validate data constraints"""
        if (self.high_quality_tokens + self.synthetic_tokens + 
            self.validation_tokens > self.total_tokens):
            logger.warning("Token allocation exceeds total budget")

@dataclass
class PerformanceTarget:
    """Performance targets and constraints"""
    target_accuracy: float = 0.95
    max_inference_latency_ms: float = 100.0
    max_memory_usage_gb: float = 16.0
    target_throughput_samples_per_sec: float = 100.0
    min_energy_efficiency: float = 0.8
    
    # Quality targets
    target_perplexity: Optional[float] = None
    target_bleu_score: Optional[float] = None
    target_f1_score: Optional[float] = None

@dataclass
class ModelArchitecture:
    """Neural network architecture specification"""
    num_parameters: int
    num_layers: int
    hidden_dim: int
    num_heads: int
    vocab_size: int
    context_length: int
    
    # Advanced architecture features
    use_moe: bool = False
    num_experts: int = 8
    expert_capacity: float = 1.25
    
    # Efficiency features
    use_flash_attention: bool = True
    use_gradient_checkpointing: bool = True
    activation_function: str = "swish"
    
    def estimate_flops_per_token(self) -> float:
        """Estimate FLOPs per token for this architecture"""
        # Transformer FLOPs approximation: 6N (forward) + 12N (backward)
        base_flops = 18 * self.num_parameters
        
        # MoE adjustment
        if self.use_moe:
            # Only a fraction of experts are active
            active_params = self.num_parameters * (2 / self.num_experts)
            base_flops = 18 * active_params
        
        # Attention FLOPs: 4 * seq_len * hidden_dim^2
        attention_flops = 4 * self.context_length * (self.hidden_dim ** 2) * self.num_layers
        
        total_flops = base_flops + attention_flops
        
        return total_flops
    
    def estimate_memory_usage(self) -> float:
        """Estimate memory usage in GB"""
        # Parameter memory (FP16)
        param_memory = self.num_parameters * 2 / (1024**3)
        
        # Activation memory (batch_size=1, FP16)
        activation_memory = (
            self.num_layers * self.context_length * self.hidden_dim * 2 / (1024**3)
        )
        
        # Optimizer states (Adam: 8 bytes per parameter)
        optimizer_memory = self.num_parameters * 8 / (1024**3)
        
        total_memory = param_memory + activation_memory + optimizer_memory
        
        # Gradient checkpointing reduces activation memory by sqrt(layers)
        if self.use_gradient_checkpointing:
            total_memory *= (1 - 0.5 * math.sqrt(self.num_layers) / self.num_layers)
        
        return total_memory

class ScalingLaws:
    """Implementation of various neural scaling laws"""
    
    @staticmethod
    def chinchilla_optimal_size(compute_budget: float) -> int:
        """
        Chinchilla scaling law: optimal model size for given compute budget
        N* = 1.3 * (C / 6)^0.5
        """
        optimal_params = int(1.3 * (compute_budget / 6) ** 0.5)
        
        # Ensure reasonable bounds
        optimal_params = max(1_000_000, min(optimal_params, 500_000_000_000))
        
        return optimal_params
    
    @staticmethod
    def chinchilla_optimal_tokens(compute_budget: float, num_params: int) -> int:
        """
        Optimal number of training tokens for given model size and compute
        D* = C / (6 * N)
        """
        optimal_tokens = int(compute_budget / (6 * num_params))
        
        # Ensure minimum training data
        optimal_tokens = max(1_000_000, optimal_tokens)
        
        return optimal_tokens
    
    @staticmethod
    def kaplan_loss_prediction(num_params: int, num_tokens: int, 
                              alpha: float = 0.076, beta: float = 0.095) -> float:
        """
        Kaplan scaling law: predict loss based on parameters and data
        L = (N/N_c)^(-Î±) + (D/D_c)^(-Î²)
        """
        # Critical values (empirically determined)
        N_c = 8.8e6  # Critical parameter count
        D_c = 5.4e6  # Critical token count
        
        param_loss = (num_params / N_c) ** (-alpha)
        data_loss = (num_tokens / D_c) ** (-beta)
        
        total_loss = param_loss + data_loss
        
        return total_loss
    
    @staticmethod
    def palm_performance_scaling(num_params: int, data_quality_score: float, 
                                compute_efficiency: float) -> float:
        """
        PaLM-style performance scaling with quality factors
        Performance âˆ N^a * Q^b * E^c
        """
        # Empirical exponents
        a, b, c = 0.32, 0.28, 0.15
        
        performance = (
            (num_params / 1e9) ** a *
            data_quality_score ** b *
            compute_efficiency ** c
        )
        
        return performance

class NeuralScalingOptimizer:
    """Advanced neural scaling laws optimizer"""
    
    def __init__(self, compute_budget: ComputeBudget, data_budget: DataBudget, 
                 performance_target: PerformanceTarget):
        self.compute_budget = compute_budget
        self.data_budget = data_budget
        self.performance_target = performance_target
        
        # Optimization history
        self.optimization_history = []
        self.pareto_front = []
        
        # Performance predictor (Gaussian Process)
        self.performance_predictor = GaussianProcessRegressor(
            kernel=Matern(length_scale=1.0, nu=2.5),
            alpha=1e-6,
            normalize_y=True
        )
        
        # Scaling laws instance
        self.scaling_laws = ScalingLaws()
        
        logger.info("ðŸ”¬ Neural Scaling Optimizer initialized")
        logger.info(f"   Compute Budget: {compute_budget.total_flops:.2e} FLOPs")
        logger.info(f"   Data Budget: {data_budget.total_tokens:.2e} tokens")
        logger.info(f"   Target Accuracy: {performance_target.target_accuracy:.3f}")
    
    def optimize_architecture(self, 
                            optimization_method: str = "multi_objective",
                            num_trials: int = 100) -> ModelArchitecture:
        """
        Optimize model architecture using scaling laws and performance targets
        
        Args:
            optimization_method: "chinchilla", "multi_objective", or "pareto"
            num_trials: Number of optimization trials
            
        Returns:
            Optimized ModelArchitecture
        """
        logger.info(f"ðŸŽ¯ Starting architecture optimization with {optimization_method}")
        
        if optimization_method == "chinchilla":
            return self._optimize_chinchilla()
        elif optimization_method == "multi_objective":
            return self._optimize_multi_objective(num_trials)
        elif optimization_method == "pareto":
            return self._optimize_pareto_front(num_trials)
        else:
            raise ValueError(f"Unknown optimization method: {optimization_method}")
    
    def _optimize_chinchilla(self) -> ModelArchitecture:
        """Optimize using pure Chinchilla scaling laws"""
        # Get optimal model size
        optimal_params = self.scaling_laws.chinchilla_optimal_size(
            self.compute_budget.training_flops
        )
        
        # Get optimal training tokens
        optimal_tokens = self.scaling_laws.chinchilla_optimal_tokens(
            self.compute_budget.training_flops, optimal_params
        )
        
        # Design architecture for optimal parameter count
        architecture = self._design_architecture_for_params(optimal_params)
        
        logger.info(f"âœ… Chinchilla-optimal architecture:")
        logger.info(f"   Parameters: {optimal_params:,}")
        logger.info(f"   Optimal tokens: {optimal_tokens:,}")
        logger.info(f"   Architecture: {architecture.num_layers}L/{architecture.hidden_dim}H")
        
        return architecture
    
    def _optimize_multi_objective(self, num_trials: int) -> ModelArchitecture:
        """Multi-objective optimization balancing accuracy, speed, and memory"""
        
        def objective(trial):
            """Optuna objective function"""
            # Sample architecture parameters
            num_layers = trial.suggest_int("num_layers", 12, 96)
            hidden_dim = trial.suggest_int("hidden_dim", 512, 8192, step=256)
            num_heads = trial.suggest_categorical("num_heads", [8, 12, 16, 20, 24, 32])
            context_length = trial.suggest_categorical("context_length", [1024, 2048, 4096, 8192])
            
            # Ensure num_heads divides hidden_dim
            while hidden_dim % num_heads != 0:
                hidden_dim += 1
            
            # Calculate parameters
            vocab_size = 50000  # Typical vocabulary size
            num_params = self._estimate_parameters(num_layers, hidden_dim, vocab_size, context_length)
            
            # Create architecture
            architecture = ModelArchitecture(
                num_parameters=num_params,
                num_layers=num_layers,
                hidden_dim=hidden_dim,
                num_heads=num_heads,
                vocab_size=vocab_size,
                context_length=context_length,
                use_moe=trial.suggest_categorical("use_moe", [True, False]),
                use_flash_attention=True,
                use_gradient_checkpointing=True
            )
            
            # Evaluate architecture
            metrics = self._evaluate_architecture(architecture)
            
            # Multi-objective score (lower is better for Optuna)
            accuracy_penalty = max(0, self.performance_target.target_accuracy - metrics['predicted_accuracy'])
            latency_penalty = max(0, metrics['inference_latency'] - self.performance_target.max_inference_latency_ms)
            memory_penalty = max(0, metrics['memory_usage'] - self.performance_target.max_memory_usage_gb)
            
            # Weighted penalty
            total_penalty = (
                accuracy_penalty * 10.0 +  # Accuracy is most important
                latency_penalty * 0.01 +   # Latency penalty
                memory_penalty * 0.1       # Memory penalty
            )
            
            return total_penalty
        
        # Run optimization
        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=num_trials)
        
        # Get best architecture
        best_params = study.best_params
        best_architecture = self._params_to_architecture(best_params)
        
        logger.info(f"âœ… Multi-objective optimization completed:")
        logger.info(f"   Best score: {study.best_value:.6f}")
        logger.info(f"   Parameters: {best_architecture.num_parameters:,}")
        logger.info(f"   Architecture: {best_architecture.num_layers}L/{best_architecture.hidden_dim}H")
        
        return best_architecture
    
    def _optimize_pareto_front(self, num_trials: int) -> List[ModelArchitecture]:
        """Find Pareto-optimal architectures"""
        logger.info("ðŸŽ¯ Computing Pareto front of optimal architectures")
        
        candidates = []
        
        # Generate diverse architecture candidates
        for _ in range(num_trials):
            # Random architecture sampling
            num_layers = np.random.randint(6, 48)
            hidden_dim = np.random.choice([512, 768, 1024, 1536, 2048, 3072, 4096])
            num_heads = np.random.choice([8, 12, 16, 20, 24])
            
            # Ensure divisibility
            while hidden_dim % num_heads != 0:
                num_heads = np.random.choice([8, 12, 16, 20, 24])
            
            vocab_size = 50000
            context_length = np.random.choice([1024, 2048, 4096])
            
            num_params = self._estimate_parameters(num_layers, hidden_dim, vocab_size, context_length)
            
            architecture = ModelArchitecture(
                num_parameters=num_params,
                num_layers=num_layers,
                hidden_dim=hidden_dim,
                num_heads=num_heads,
                vocab_size=vocab_size,
                context_length=context_length,
                use_moe=np.random.choice([True, False]),
                use_flash_attention=True,
                use_gradient_checkpointing=True
            )
            
            metrics = self._evaluate_architecture(architecture)
            candidates.append((architecture, metrics))
        
        # Find Pareto front
        pareto_front = self._compute_pareto_front(candidates)
        
        logger.info(f"âœ… Found {len(pareto_front)} Pareto-optimal architectures")
        
        # Return the best balanced architecture from Pareto front
        best_architecture = self._select_best_from_pareto(pareto_front)
        
        self.pareto_front = pareto_front
        return best_architecture
    
    def _evaluate_architecture(self, architecture: ModelArchitecture) -> Dict[str, float]:
        """Evaluate architecture performance using scaling laws"""
        
        # Predicted accuracy using scaling laws
        predicted_loss = self.scaling_laws.kaplan_loss_prediction(
            architecture.num_parameters,
            self.data_budget.total_tokens
        )
        predicted_accuracy = 1.0 / (1.0 + predicted_loss)  # Convert loss to accuracy
        
        # Performance scaling
        data_quality = 0.9  # Assume high-quality data
        compute_efficiency = 0.8  # Reasonable efficiency
        performance_score = self.scaling_laws.palm_performance_scaling(
            architecture.num_parameters, data_quality, compute_efficiency
        )
        
        # Adjust accuracy prediction with performance scaling
        predicted_accuracy = min(0.99, predicted_accuracy * (1 + 0.1 * performance_score))
        
        # Estimate inference latency
        flops_per_token = architecture.estimate_flops_per_token()
        # Assume A100 GPU: ~300 TFLOPS for inference
        gpu_throughput = 300e12  # FLOPS/sec
        inference_latency = (flops_per_token / gpu_throughput) * 1000  # ms
        
        # Memory usage
        memory_usage = architecture.estimate_memory_usage()
        
        # Training time estimate
        total_training_flops = flops_per_token * self.data_budget.total_tokens
        training_time_hours = total_training_flops / (gpu_throughput * 3600)
        
        return {
            'predicted_accuracy': predicted_accuracy,
            'predicted_loss': predicted_loss,
            'performance_score': performance_score,
            'inference_latency': inference_latency,
            'memory_usage': memory_usage,
            'training_time_hours': training_time_hours,
            'flops_per_token': flops_per_token
        }
    
    def _design_architecture_for_params(self, target_params: int) -> ModelArchitecture:
        """Design architecture to achieve target parameter count"""
        
        # Typical ratios for transformer architectures
        vocab_size = 50000
        context_length = 4096
        
        # Use empirical relationships for GPT-style models
        # N â‰ˆ 12 * L * d^2 (where L=layers, d=hidden_dim)
        # Try different layer counts to find good architecture
        
        best_architecture = None
        best_error = float('inf')
        
        for num_layers in range(12, 96, 6):
            # Solve for hidden_dim: d = sqrt(N / (12 * L))
            hidden_dim = int(math.sqrt(target_params / (12 * num_layers)))
            
            # Round to nearest multiple of head count
            for num_heads in [8, 12, 16, 20, 24, 32]:
                rounded_dim = (hidden_dim // num_heads) * num_heads
                if rounded_dim < 512:
                    continue
                
                actual_params = self._estimate_parameters(
                    num_layers, rounded_dim, vocab_size, context_length
                )
                
                error = abs(actual_params - target_params) / target_params
                
                if error < best_error:
                    best_error = error
                    best_architecture = ModelArchitecture(
                        num_parameters=actual_params,
                        num_layers=num_layers,
                        hidden_dim=rounded_dim,
                        num_heads=num_heads,
                        vocab_size=vocab_size,
                        context_length=context_length,
                        use_moe=False,
                        use_flash_attention=True,
                        use_gradient_checkpointing=True
                    )
        
        return best_architecture
    
    def _estimate_parameters(self, num_layers: int, hidden_dim: int, 
                           vocab_size: int, context_length: int) -> int:
        """Estimate parameter count for transformer architecture"""
        
        # Embedding layer
        embedding_params = vocab_size * hidden_dim
        
        # Position embeddings
        position_params = context_length * hidden_dim
        
        # Transformer layers
        # Each layer: attention (4 * d^2) + MLP (8 * d^2) + layer norms (4 * d)
        params_per_layer = (
            4 * hidden_dim * hidden_dim +  # Attention projections
            8 * hidden_dim * hidden_dim +  # MLP weights  
            4 * hidden_dim                 # Layer norms
        )
        
        transformer_params = num_layers * params_per_layer
        
        # Output layer (tied with input embedding)
        output_params = 0  # Assuming tied weights
        
        # Final layer norm
        final_norm_params = hidden_dim
        
        total_params = (
            embedding_params + position_params + transformer_params + 
            output_params + final_norm_params
        )
        
        return int(total_params)
    
    def _params_to_architecture(self, params: Dict[str, Any]) -> ModelArchitecture:
        """Convert Optuna parameters to ModelArchitecture"""
        
        num_layers = params["num_layers"]
        hidden_dim = params["hidden_dim"]
        num_heads = params["num_heads"]
        context_length = params["context_length"]
        use_moe = params["use_moe"]
        
        # Ensure divisibility
        while hidden_dim % num_heads != 0:
            hidden_dim += 1
        
        vocab_size = 50000
        num_params = self._estimate_parameters(num_layers, hidden_dim, vocab_size, context_length)
        
        return ModelArchitecture(
            num_parameters=num_params,
            num_layers=num_layers,
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            vocab_size=vocab_size,
            context_length=context_length,
            use_moe=use_moe,
            use_flash_attention=True,
            use_gradient_checkpointing=True
        )
    
    def _compute_pareto_front(self, candidates: List[Tuple[ModelArchitecture, Dict]]) -> List[Tuple[ModelArchitecture, Dict]]:
        """Compute Pareto front of architectures"""
        
        pareto_front = []
        
        for i, (arch1, metrics1) in enumerate(candidates):
            is_dominated = False
            
            for j, (arch2, metrics2) in enumerate(candidates):
                if i == j:
                    continue
                
                # Check if arch2 dominates arch1
                # arch2 dominates if it's better in all objectives
                better_accuracy = metrics2['predicted_accuracy'] >= metrics1['predicted_accuracy']
                better_latency = metrics2['inference_latency'] <= metrics1['inference_latency']
                better_memory = metrics2['memory_usage'] <= metrics1['memory_usage']
                
                # At least one strictly better
                strictly_better = (
                    metrics2['predicted_accuracy'] > metrics1['predicted_accuracy'] or
                    metrics2['inference_latency'] < metrics1['inference_latency'] or
                    metrics2['memory_usage'] < metrics1['memory_usage']
                )
                
                if better_accuracy and better_latency and better_memory and strictly_better:
                    is_dominated = True
                    break
            
            if not is_dominated:
                pareto_front.append((arch1, metrics1))
        
        return pareto_front
    
    def _select_best_from_pareto(self, pareto_front: List[Tuple[ModelArchitecture, Dict]]) -> ModelArchitecture:
        """Select best architecture from Pareto front based on preferences"""
        
        if not pareto_front:
            raise ValueError("Empty Pareto front")
        
        best_score = -float('inf')
        best_architecture = None
        
        for architecture, metrics in pareto_front:
            # Weighted score based on preferences
            score = (
                metrics['predicted_accuracy'] * 5.0 +  # Accuracy is most important
                (1.0 / (1.0 + metrics['inference_latency'] / 100.0)) * 2.0 +  # Latency
                (1.0 / (1.0 + metrics['memory_usage'] / 16.0)) * 1.0  # Memory
            )
            
            if score > best_score:
                best_score = score
                best_architecture = architecture
        
        return best_architecture
    
    def predict_performance(self, architecture: ModelArchitecture) -> Dict[str, float]:
        """Predict performance for given architecture"""
        return self._evaluate_architecture(architecture)
    
    def generate_scaling_report(self, architecture: ModelArchitecture) -> Dict[str, Any]:
        """Generate comprehensive scaling analysis report"""
        
        metrics = self._evaluate_architecture(architecture)
        
        # Scaling law analysis
        optimal_chinchilla_size = self.scaling_laws.chinchilla_optimal_size(
            self.compute_budget.training_flops
        )
        optimal_tokens = self.scaling_laws.chinchilla_optimal_tokens(
            self.compute_budget.training_flops, architecture.num_parameters
        )
        
        # Efficiency analysis
        param_efficiency = optimal_chinchilla_size / architecture.num_parameters
        data_efficiency = optimal_tokens / self.data_budget.total_tokens
        
        # Compute utilization
        total_training_flops = metrics['flops_per_token'] * self.data_budget.total_tokens
        compute_utilization = total_training_flops / self.compute_budget.training_flops
        
        report = {
            'architecture': {
                'parameters': architecture.num_parameters,
                'layers': architecture.num_layers,
                'hidden_dim': architecture.hidden_dim,
                'heads': architecture.num_heads,
                'context_length': architecture.context_length,
                'use_moe': architecture.use_moe
            },
            'performance_predictions': metrics,
            'scaling_analysis': {
                'chinchilla_optimal_size': optimal_chinchilla_size,
                'optimal_training_tokens': optimal_tokens,
                'parameter_efficiency': param_efficiency,
                'data_efficiency': data_efficiency,
                'compute_utilization': compute_utilization
            },
            'efficiency_scores': {
                'parameter_efficiency': min(1.0, param_efficiency),
                'data_efficiency': min(1.0, data_efficiency),
                'compute_efficiency': min(1.0, compute_utilization),
                'overall_efficiency': min(1.0, (param_efficiency + data_efficiency + compute_utilization) / 3)
            },
            'recommendations': self._generate_recommendations(architecture, metrics)
        }
        
        return report
    
    def _generate_recommendations(self, architecture: ModelArchitecture, 
                                metrics: Dict[str, float]) -> List[str]:
        """Generate optimization recommendations"""
        recommendations = []
        
        # Parameter count recommendations
        optimal_size = self.scaling_laws.chinchilla_optimal_size(self.compute_budget.training_flops)
        if architecture.num_parameters > optimal_size * 1.5:
            recommendations.append(
                f"Model is oversized ({architecture.num_parameters:,} vs optimal {optimal_size:,}). "
                "Consider reducing layers or hidden dimension."
            )
        elif architecture.num_parameters < optimal_size * 0.5:
            recommendations.append(
                f"Model is undersized ({architecture.num_parameters:,} vs optimal {optimal_size:,}). "
                "Consider increasing layers or hidden dimension."
            )
        
        # Memory recommendations
        if metrics['memory_usage'] > self.performance_target.max_memory_usage_gb:
            recommendations.append(
                f"Memory usage ({metrics['memory_usage']:.1f}GB) exceeds target "
                f"({self.performance_target.max_memory_usage_gb:.1f}GB). "
                "Consider gradient checkpointing or model sharding."
            )
        
        # Latency recommendations
        if metrics['inference_latency'] > self.performance_target.max_inference_latency_ms:
            recommendations.append(
                f"Inference latency ({metrics['inference_latency']:.1f}ms) exceeds target "
                f"({self.performance_target.max_inference_latency_ms:.1f}ms). "
                "Consider using Flash Attention or reducing model size."
            )
        
        # MoE recommendations
        if not architecture.use_moe and architecture.num_parameters > 10_000_000_000:
            recommendations.append(
                "For large models (>10B parameters), consider Mixture of Experts "
                "to improve efficiency while maintaining capacity."
            )
        
        # Data efficiency
        optimal_tokens = self.scaling_laws.chinchilla_optimal_tokens(
            self.compute_budget.training_flops, architecture.num_parameters
        )
        if self.data_budget.total_tokens < optimal_tokens * 0.5:
            recommendations.append(
                f"Training data ({self.data_budget.total_tokens:,} tokens) is insufficient. "
                f"Optimal amount: {optimal_tokens:,} tokens. Consider data augmentation."
            )
        
        return recommendations
    
    def save_optimization_results(self, architecture: ModelArchitecture, 
                                output_path: str = "optimization_results.json"):
        """Save optimization results to file"""
        
        report = self.generate_scaling_report(architecture)
        report['timestamp'] = datetime.now().isoformat()
        report['compute_budget'] = {
            'total_flops': self.compute_budget.total_flops,
            'memory_gb': self.compute_budget.memory_gb,
            'gpu_count': self.compute_budget.gpu_count,
            'gpu_type': self.compute_budget.gpu_type
        }
        report['data_budget'] = {
            'total_tokens': self.data_budget.total_tokens,
            'high_quality_tokens': self.data_budget.high_quality_tokens
        }
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"ðŸ’¾ Optimization results saved to {output_path}")

# Factory functions and utilities

def create_scaling_optimizer_for_astrobiology() -> NeuralScalingOptimizer:
    """Create pre-configured optimizer for astrobiology workloads"""
    
    # Define realistic compute budget for research lab
    compute_budget = ComputeBudget(
        total_flops=1e18,  # 1 exaFLOP
        training_flops=8e17,  # 80% for training
        inference_flops=2e17,  # 20% for inference  
        memory_gb=80.0,  # Multiple A100s
        time_hours=168,  # 1 week
        gpu_count=8,
        gpu_type="A100"
    )
    
    # Define data budget based on astrobiology datasets
    data_budget = DataBudget(
        total_tokens=100_000_000,  # 100M tokens
        high_quality_tokens=80_000_000,  # 80M high-quality
        synthetic_tokens=15_000_000,  # 15M synthetic
        validation_tokens=5_000_000,  # 5M validation
        domains=["astrobiology", "climate", "spectroscopy", "genomics", "astronomy"]
    )
    
    # Define performance targets for astrobiology tasks
    performance_target = PerformanceTarget(
        target_accuracy=0.96,  # 96% accuracy target
        max_inference_latency_ms=100.0,  # 100ms max latency
        max_memory_usage_gb=32.0,  # 32GB max memory
        target_throughput_samples_per_sec=50.0,  # 50 samples/sec
        min_energy_efficiency=0.8  # 80% efficiency
    )
    
    optimizer = NeuralScalingOptimizer(compute_budget, data_budget, performance_target)
    
    return optimizer

async def demonstrate_scaling_optimization():
    """Demonstrate the neural scaling optimizer"""
    logger.info("ðŸš€ Demonstrating Neural Scaling Laws Optimization")
    
    # Create optimizer
    optimizer = create_scaling_optimizer_for_astrobiology()
    
    # Run optimization
    logger.info("ðŸ“Š Running Chinchilla optimization...")
    chinchilla_arch = optimizer.optimize_architecture("chinchilla")
    
    logger.info("ðŸŽ¯ Running multi-objective optimization...")
    multi_obj_arch = optimizer.optimize_architecture("multi_objective", num_trials=50)
    
    # Generate reports
    chinchilla_report = optimizer.generate_scaling_report(chinchilla_arch)
    multi_obj_report = optimizer.generate_scaling_report(multi_obj_arch)
    
    # Save results
    optimizer.save_optimization_results(chinchilla_arch, "chinchilla_results.json")
    optimizer.save_optimization_results(multi_obj_arch, "multi_objective_results.json")
    
    logger.info("âœ… Scaling optimization demonstration completed")
    
    return {
        'chinchilla_architecture': chinchilla_arch,
        'multi_objective_architecture': multi_obj_arch,
        'chinchilla_report': chinchilla_report,
        'multi_objective_report': multi_obj_report
    }

if __name__ == "__main__":
    asyncio.run(demonstrate_scaling_optimization()) 