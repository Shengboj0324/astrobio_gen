defaults:
  - _self_
  - model: datacube_unet
  - trainer: gpu_production
  - data: datacube

# Advanced datacube configuration
datacube:
  # Primary zarr store path (environment variable supported)
  zarr_root: "${oc.env:ASTRO_CUBE_ZARR,s3://astrobio-zarr-cubes-20250714}"
  
  # Backup/additional zarr stores for ensemble processing
  additional_zarr_stores: []
  
  # Cloud-first storage configuration
  storage_mode: "cloud_first"  # Options: local, cloud_first, cloud_only
  local_cache_size_gb: 5       # Small local cache for active processing
  temp_download_path: "temp/"  # Temporary local processing space
  
  # Variables configuration
  input_variables:
    - "T_surf"      # Surface temperature
    - "q_H2O"       # Water vapor mixing ratio
    - "cldfrac"     # Cloud fraction
    - "albedo"      # Surface albedo
    - "psurf"       # Surface pressure
  
  output_variables:
    - "T_surf"
    - "q_H2O"
    - "cldfrac"
    - "albedo"
    - "psurf"
  
  # Chunking strategy for optimal performance
  chunking:
    adaptive: true
    base_chunks:
      lat: 40
      lon: 40
      lev: 15
      time: 4
    memory_limit: "2GB"
    
  # Spatial and temporal configuration
  spatial:
    resolution: "auto"  # auto, high, medium, low
    crop_region: null   # [lat_min, lat_max, lon_min, lon_max]
    
  temporal:
    window_size: 10
    stride: 5
    seasonal_normalization: true
    
  # Advanced processing options
  processing:
    normalize_globally: true
    remove_climatology: false
    apply_physics_constraints: true
    quality_filtering: true
    
  # Caching and performance
  caching:
    enabled: true
    cache_dir: "data/cache/cube_cache"
    max_cache_size: "10GB"
    
# Model configuration
model:
  type: "datacube"
  mode: "datacube"  # scalar, datacube, joint, spectral
  
# Data configuration  
data:
  root: "${datacube.zarr_root}"
  batch_size: 4
  num_workers: 6
  pin_memory: true
  
# Training configuration
trainer:
  max_epochs: 120
  precision: "16-mixed"
  accelerator: "gpu"
  devices: "auto"
  strategy: "ddp"
  
# Surrogate configuration
surrogate:
  mode: "datacube"
  export_format: "onnx"
  weights_path: "models/surrogate_cube.onnx"
  
# API configuration
api:
  enable_streaming: true
  max_cube_size: "1GB"
  compression: "lz4"
  
# Paths configuration
paths:
  cube_zarr: "${datacube.zarr_root}"
  model_checkpoints: "models/checkpoints"
  logs: "logs"
  cache: "${datacube.caching.cache_dir}"

# Database Configuration (Centralized SQLite Management)
database:
  # Base data directory for all databases
  base_path: "data"
  
  # Core system databases with consistent paths
  databases:
    # Main metadata database (primary)
    metadata:
      path: "data/metadata/metadata.db"
      description: "Main scientific metadata and dataset registry"
      tables: ["datasets", "experiments", "data_chunks", "predictions", "astronomical_objects"]
      
    # Data versioning and provenance
    versions:
      path: "data/versions/versions.db" 
      description: "Data versioning and provenance tracking"
      tables: ["datasets", "versions", "changes", "provenance", "branches"]
      
    # Quality monitoring and validation
    quality:
      path: "data/quality/quality_monitor.db"
      description: "Data quality reports and monitoring"
      tables: ["quality_reports", "quality_issues", "quality_trends"]
      
    # Security and access control
    security:
      path: "data/metadata/security.db"
      description: "Security, encryption, and access logging"
      tables: ["file_metadata", "access_log", "security_alerts"]
      
    # Pipeline state and execution tracking
    pipeline:
      path: "data/pipeline/pipeline_state.db"
      description: "Automated pipeline execution state"
      tables: ["pipeline_runs", "task_executions"]
      
    # Domain-specific databases
    kegg:
      path: "data/processed/kegg/kegg_database.db"
      description: "KEGG pathway and metabolic data"
      tables: ["pathways", "reactions", "compounds", "organisms"]
      
    agora2:
      path: "data/processed/agora2/metabolic_models.db"
      description: "AGORA2 metabolic models and genome data"
      tables: ["agora2_models", "reactions", "metabolites", "genes", "ncbi_genomes"]
      
    gems:
      path: "data/processed/jgi/gems.db"
      description: "JGI genome annotation and functional analysis"
      tables: ["genomes", "annotations", "functional_categories"]
      
    gtdb:
      path: "data/processed/gtdb/gtdb.db"
      description: "GTDB taxonomic and phylogenetic data"
      tables: ["genomes", "taxonomy", "trees", "metadata"]
      
    uniprot:
      path: "data/processed/uniprot/uniprot.db"
      description: "UniProt protein sequences and annotations"
      tables: ["proteins", "annotations", "cross_references"]
      
    ahed:
      path: "data/processed/nasa/ahed.db"
      description: "NASA astrobiology and habitability data"
      tables: ["datasets", "analyses", "habitability_metrics"]
  
  # Database configuration settings
  settings:
    # SQLite configuration
    sqlite:
      journal_mode: "WAL"  # Write-Ahead Logging for better performance
      synchronous: "NORMAL"  # Balance between safety and speed
      cache_size: 10000  # Cache size in pages (roughly 40MB)
      temp_store: "MEMORY"  # Store temporary tables in memory
      foreign_keys: true  # Enable foreign key constraints
      
    # Connection pooling
    connection:
      max_connections: 10
      pool_timeout: 30
      pool_recycle: 3600  # Recycle connections every hour
      
    # Backup and maintenance
    maintenance:
      auto_vacuum: "INCREMENTAL"  # Automatic space reclamation
      backup_interval: "daily"  # Daily backup schedule
      optimize_interval: "weekly"  # Weekly VACUUM and ANALYZE
      checkpoint_interval: 1000  # WAL checkpoint after 1000 transactions
      
    security:
      encryption_enabled: false  # Enable for sensitive data
      access_logging: true  # Log all database access
      permission_checks: true  # Enforce file permissions
      backup_encryption: false  # Encrypt backup files

# AWS Configuration
aws:
  region: "us-east-1"
  
  # S3 Buckets (configured and verified - July 17, 2025)
  s3_buckets:
    primary: "astrobio-data-primary-20250717"  # Main data storage
    backup: "astrobio-data-backup-20250717"    # Cross-region backup
    zarr: "astrobio-zarr-cubes-20250717"       # Zarr datacubes
    logs: "astrobio-logs-metadata-20250717"    # Logs and metadata
  
  # Compute Configuration
  compute:
    dev_instance_type: "t3.large"
    training_instance_type: "g4dn.xlarge"  # GPU for ML training
    batch_instance_type: "c5.4xlarge"      # CPU for data processing
  
  # Cost Management
  cost_management:
    daily_budget_alert: 100   # USD
    monthly_budget_limit: 1000  # USD
    auto_shutdown_idle: true
    lifecycle_policies: true   # Auto-archive old data
  
  # Storage Configuration
  storage:
    mode: "cloud_first"              # Store data directly to S3
    local_cache_gb: 5                # Small local cache (5GB max)
    temp_processing_gb: 2            # Temporary space for processing
    immediate_upload: true           # Upload processed data immediately
    cleanup_local_after_upload: true # Delete local files after S3 upload